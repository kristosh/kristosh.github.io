<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://kristosh.github.io//feed.xml" rel="self" type="application/atom+xml" /><link href="https://kristosh.github.io//" rel="alternate" type="text/html" hreflang="en" /><updated>2024-05-16T13:32:23+00:00</updated><id>https://kristosh.github.io//feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics.
</subtitle><entry><title type="html">Why should I trust you? Local Interpretable Model-agnostic Explanations</title><link href="https://kristosh.github.io//blog/2023/LIME/" rel="alternate" type="text/html" title="Why should I trust you? Local Interpretable Model-agnostic Explanations" /><published>2023-05-13T00:00:00+00:00</published><updated>2023-05-13T00:00:00+00:00</updated><id>https://kristosh.github.io//blog/2023/LIME</id><content type="html" xml:base="https://kristosh.github.io//blog/2023/LIME/"><![CDATA[<h1 id="local-interpretable-model-agnostic-explanations">Local Interpretable Model-agnostic Explanations</h1>

<center>
<video autoplay="" muted="" loop="" controls="" src="https://kristosh.github.io//assets/video/2023-05-13-LIME/caretta.mp4" style="width:600px" type="video/mp4">
</video>
<figcaption>A demonstration of the whole LIME algorithm.</figcaption>
</center>

<p>In this post, we will study how
<mark style="background-color:LavenderBlush;">LIME</mark> (Local
Interpretable Model-agnostic Explanations) ([1]) generates
explanations for image classification tasks. The basic idea is to
understand why a machine learning model predicts that a specific image
belongs to a certain class (<em>caretta</em> our visual example).
Briefly, this technique constructs a <em>new</em> simple model (for
example a <mark style="background-color:LavenderBlush;"><em>linear
classifier</em></mark>) which is easy to be interpreted by humans and at the same time approximates
the predictions of the <em>black-box</em> model in the neighborhood
around the instance that needs to be explained (<em>local faithfulness
</em>).</p>

<p>The <mark style="background-color:LavenderBlush;">LIME</mark> explainer is <em>model-agnostic</em> which means is not
restricted to a specific model and can be used to explain any
<mark>black-box</mark> classifier. So we don’t need to have access to
the details of our model (input, intermediate layers etc) to generate
explanations. Moreover, the explainer is <em>local</em> meaning that it
explains the prediction of the model in the neighborhood of the instance
being explained. This technique lies in the PostHoc category of XAI
methods, meaning that it explains the model after it has been trained.</p>

<h2 id="interpretable-representations">Interpretable Representations</h2>

<p>An interpretable explanation in an image classifier explainer should use
a representation that is understandable to humans, by explaining which
parts of the input image influence the model decision. For instance, the
pixel-based representations are not very informative especially when we
deal with huge images and therefore a better way to explain the model
decision is to use
<a href="https://infoscience.epfl.ch/record/149300">super-pixels</a> ([3]). Super-pixels
are groups of pixels that share similar characteristics such as color
and texture. Hence, a possible interpretable representation for image
classification may be a binary vector indicating the <em>presence</em>
or <em>absence</em> of a super-pixel.</p>

<p>Thus, our explainer needs to find a way to attribute importance to each
super-pixel in the initial input image. It’s important to note here,
that the interpretable representations are meant to be just for the
<mark style="background-color:LightCyan;">LIME</mark> explainer while the <em>black-box</em> can still be trained using the
original pixel-based representations.</p>

<p><mark style="background-color:LightCyan;">LIME</mark> approach aims to
just explain why the classifier took a specific decision upon a specific
input image. It does not aim to explain the whole model. Authors, in the
paper, proposed a mechanism called
<mark style="background-color:LightCyan;">SP-LIME</mark> that aims to
explain the whole model. While we will not touch this method in this
tutorial we encourage you to have a look at it in the original paper.An interpretable explanation in an image classifier explainer should use
a representation that is understandable to humans, by explaining which
parts of the input image influence the model decision. For instance, the
pixel-based representations are not very informative especially when we
deal with huge images and therefore a better way to explain the model
decision is to use
<a href="https://infoscience.epfl.ch/record/149300">super-pixels</a> ([3]). Super-pixels
are groups of pixels that share similar characteristics such as color
and texture. Hence, a possible interpretable representation for image
classification may be a binary vector indicating the <em>presence</em>
or <em>absence</em> of a super-pixel.</p>

<p>Thus, our explainer needs to find a way to attribute importance to each
super-pixel in the initial input image. It’s important to note here,
that the interpretable representations are meant to be just for the
<mark style="background-color:LightCyan;">LIME</mark> explainer while the <em>black-box</em> can still be trained using the
original pixel-based representations.</p>

<p><mark style="background-color:LightCyan;">LIME</mark> approach aims to
just explain why the classifier took a specific decision upon a specific
input image. It does not aim to explain the whole model. Authors, in the
paper, proposed a mechanism called
<mark style="background-color:LightCyan;">SP-LIME</mark> that aims to
explain the whole model. While we will not touch this method in this
tutorial we encourage you to have a look at it in the original paper.</p>

<h2 id="lime-approach-details">LIME approach details</h2>

<p>To explain how <mark style="background-color:LavenderBlush;">LIME</mark> works in detail we should introduce some definitions and maths
😎.</p>

<p>Hence, let $\mathbf{x} \in R^{d}$ denote the original vector
representation of an instance being explained (in our case a vector with
all pixels in the image), and we use $\mathbf{x}^{\prime} \in {0, 1}^d$ to denote a
binary vector for its interpretable representation (super-pixels).</p>

<p>The <mark style="background-color:LavenderBlush;">LIME</mark> explainer is defined as (or explanation model) $g \in G$, where $G$
is a class of potentially interpretable models, such as <em>linear
models</em>, <em>decision trees</em> etc. To keep things simple, in this
tutorial, we will consider just <em>linear classifiers</em>. As not
every $g \in G$ may be simple enough to be interpretable $\Omega(g)$ is
defined as a measure of complexity (in juxtaposition with the
interpretability) of the explanation $g \in G$. For example, for
decision trees $\Omega(g)$ may be the depth of the tree, while for
linear models, $\Omega(g)$ may be the number of non-zero weights. We
define as $f: R^{d} \to R$ the <em>black-box</em> model that we would
like to explain. In classification, $f(\mathbf{x})$ is the probability
(or a binary indicator) that $\mathbf{x}$ belongs to a certain class.</p>

<p>We further use $\pi_{\mathbf{x}}(\mathbf{z})$ as a proximity measure
between an instance $\mathbf{z}$ to $\mathbf{x}$, so as to define
locality around $\mathbf{x}$. Finally, let
$\mathcal{L}(f, g, \pi_{\mathbf{x}})$ be a measure of how unfaithful $g$
is in approximating $f$ in the locality defined by $\pi_{\mathbf{x}}$.
To ensure both <em>interpretability</em> and <em>local fidelity</em>,
they must minimize $L(f, g, \pi_{x})$ while having $\Omega(g)$ be as low
as possible. This will keep the complexity of the explanation low while
maintaining the fidelity of the explanation high.</p>

<p>Hence, the loss function for the <mark style="background-color:LavenderBlush;">LIME</mark> explainer is the following equation:</p>

\[\xi(\mathbf{x}) = \mathcal{L}(f, g, \pi_{\mathbf{x}}) + \Omega(g)\]

<p>The above equation contains the tradeoff between <em>local fidelity</em>
which is expressed by $L$ and the <em>complexity</em> of the explainer that it is expressed
by $\Omega$.</p>

<p>The first term $\mathcal{L}(f, g, \pi_{\mathbf{x}})$ in the equation is represented by the weighted square loss:</p>

\[\mathcal{L}(f, g, \pi_{\mathbf{x}}) = \sum_{\mathbf{z}, \mathbf{z}^{'}}\pi_{\mathbf{x}}(\mathbf{z})(f(\mathbf{z})- g(\mathbf{z}^{'}))^{2}\]

<p>with $\pi_{\mathbf{x}}$ to be a kernel function that measures the proximity of $z$ to $x$:</p>

\[\pi_{\mathbf{x}} =  \exp(-D(\mathbf{x},\mathbf{z})^{2}/\sigma*{2})\]

<p>The idea is that by tuning the weights $\mathbf{w}$ we can use them
directly as a feature attribution to each super-pixel. The higher the
weight that corresponds to a specific super-pixel the more important
this super-pixel is for the prediction of the <em>black-box</em> model
and vice-versa.</p>

<p>It’s important to remind you here of the terms <em>faithfulness</em>
and <em>local fidelity</em> which are about how well our explainer
$g$ can approximate the decision of the <em>black-box</em> model $f$ in
the locality defined by $\pi_{\mathbf{x}}$.</p>

<p>The whole <mark style="background-color:LavenderBlush;">LIME</mark> algorithm can be summarized as follows:</p>

<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2023-05-13-LIME/algorithm.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>The <mark style="background-color:LavenderBlush;">kernel</mark> shows the proximity between the instance that we desire to explain and the
generated samples in the neighborhood. The neighborhood is created by
<mark style="background-color:LightCyan;">sampling</mark> instances
around the initial image. The sampling is done by perturbing
the instance being explained. For example, in the case of images, we can
perturb the image by zeroing out some super-pixels. The perturbed
instances are then fed to the <em>black-box</em> model and the output is
used to train the explainer. The weights of the interpretable model are
then used to explain the prediction of the <em>black-box</em> model.
Finally, in the algorithm,
<mark style="background-color:Lavender;">K-lasso</mark> refers to the
regularization that is introduced in a previous equation and relates to
the term $\Omega(g)$.</p>

<center>
<video autoplay="" muted="" loop="" controls="" src="https://kristosh.github.io//assets/video/2023-05-13-LIME/LIME3.mp4" style="width:600px" type="video/mp4">
</video>
<figcaption>A demonstration of the whole LIME algorithm.</figcaption>
</center>

<p>The above video explains the whole <mark style="background-color:LavenderBlush;">LIME</mark> process. The initial surface
represents the <mark style="background-color:Lavender;">black-box</mark>
classifier and the regions for the class of interest
(e.g. Caretta-caretta with the light-pink color). The dark red-colored
dot denotes the sample that we would like to explain and it is an image
with the label <em> Caretta-caretta</em>. The first step is to sample
the neighborhood of the point $\mathbf{x}$ that we would like to
explain. Several points are generated. The size of each generated sample
and the transparency relates to the distance from the initial point
$\mathbf{x}$ which is calculated based using
$\pi_{\mathbf{x}}(\mathbf{z})$. The next step is to apply the
<mark style="background-color:Lavender;">black-box</mark> classifier
$f()$ to find the label for each generated point. Samples with red
represent the class Caretta-caretta while samples with purple represent
the adversary class (not Caretta-caretta ;). The next step is to train the
interpretable model $g()$ using the generated samples. The weights of
the interpretable model are used to explain the prediction of the
<mark style="background-color:Lavender;">black-box</mark> classifier.</p>

<h2 id="code-implementation">Code implementation</h2>

<p>Firstly, we will need to import the required libraries. The code is written in Python 3.6.9 and PyTorch 1.7.0. You can access the code in google colab using the following <a href="TBA">link</a>. Firstly, you should run the following command to install the necessary libraries:</p>

<pre><code class="language-ptyhon">conda env create -f my_lime.yml
conda activate my_lime
</code></pre>

<h3 id="imports">Imports</h3>

<p>Now, having installed the necessary libraries we can import them. Throughout the whole tutorial, we will make use of The following libraries are used:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">os</span><span class="p">,</span> <span class="n">json</span>
<span class="kn">import</span> <span class="n">cv2</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">models</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="n">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="n">copy</span>

<span class="kn">import</span> <span class="n">sklearn</span>
<span class="kn">import</span> <span class="n">sklearn.metrics</span>
<span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="initialization-of-a-pre-trained-vgg19-model">Initialization of a pre-trained VGG19 model</h3>
<p>The very first thing that we will do is load a pre-trained VGG19 model. This model will be used to classify images and we will try to explain
its behavior. The output of the model is a vector of 1000 probabilities
belonging to each class from the ImageNet dataset. The model is
initialized and the weights are loaded. The model is set to evaluation
mode. The model is set to run on GPU if available. You can do that on
Google colab by enabling the GPU option. The steps for that are the
following: <mark>Edit -\&gt; Notebook settings -\&gt; Hardware accelerator -\&gt;
GPU</mark>. The code for all these steps is the following:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre><span class="c1"># load model
# model_type = 'vgg19'
</span><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nf">vgg19</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># run it on a GPU if available:
</span><span class="n">cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda:</span><span class="sh">'</span><span class="p">,</span> <span class="n">cuda</span><span class="p">,</span> <span class="sh">'</span><span class="s">device:</span><span class="sh">'</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># set model to evaluation
</span><span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Ignore for now the warnings! This code should return the architecture of
the VGG19 model. Of course, feel free to choose the model of your choice
(the code should work with any model). Now let’s load and process the
image (for the VGG19 classifier) that we would like to test our
<mark style="background-color:Lavender;">LIME</mark> explainer. You can
freely choose the image that you would like to explain.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">imread_img</span><span class="p">(</span><span class="n">file_name</span><span class="p">):</span>
  
  <span class="c1"># read the image and convert it - Set your pathto the image
</span>  <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">imread</span><span class="p">(</span><span class="sh">'</span><span class="s">caretta.png</span><span class="sh">'</span><span class="p">)</span>
  <span class="nf">if </span><span class="p">(</span><span class="nf">type</span><span class="p">(</span><span class="n">img</span><span class="p">)</span> <span class="ow">is</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">resize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">img:</span><span class="sh">'</span><span class="p">,</span> <span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">image not found - set your path to the image</span><span class="sh">'</span><span class="p">)</span>
  
  <span class="k">return</span> <span class="n">img</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>We do make use of the
<mark style="background-color:Lavender;">OpenCV</mark> library to read
the <mark style="background-color:Lavender;">caretta.png</mark> image from <mark>files/</mark>.
You can check all the provided images or download one from the web.</p>

<p>Note that you could do the same by using torchvision datasets and
transforms. We will show an example of that when we will use the
<mark style="background-color:Lavender;">LIME</mark> explainer for our
lab exercise at the end of this tutorial.</p>

<h3 id="image-pre-processing">Image pre-processing</h3>

<p>As usual, we will need to normalize our input image. The normalization
is done using the mean and standard deviation of the ImageNet dataset.
The image is also transposed to the correct tensor format:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">pre_processing</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">cuda</span><span class="p">):</span>
    <span class="c1"># Students should transpose the image to the correct tensor format. 
</span>    <span class="c1"># Students should ensure that gradient for input is calculated       
</span>    <span class="c1"># set the GPU device
</span>    <span class="k">if</span> <span class="n">cuda</span><span class="p">:</span>
        <span class="n">torch_device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda:0</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">torch_device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># normalise for ImageNet
</span>    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">]).</span><span class="nf">reshape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]).</span><span class="nf">reshape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span> <span class="o">/</span> <span class="mi">255</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="p">(</span><span class="n">obs</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span>

    <span class="c1"># make tensor format that keeps track of gradient
</span>    <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>       
    <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
    <span class="n">obs_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch_device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">obs_tensor</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>We can do the same (resizing, normalization and conversion to tensor) by
using the torchvision transform:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="n">transform</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
        <span class="n">A</span><span class="p">.</span><span class="nc">Resize</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span>
        <span class="n">A</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]),</span>
        <span class="nc">ToTensorV2</span><span class="p">()])</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Then, the next that we will do is load the image and preprocess it. We
will also check the prediction of the VGG19 model. Note that the
prediction is correct (E.g. 33 and 34 are ‘caretta-caretta’ and
‘turtle’). The code is the following:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">target_label_idx</span><span class="p">,</span> <span class="n">cuda</span><span class="p">):</span>
    <span class="c1"># Makes prediction after preprocessing image 
</span>    <span class="c1"># Note that output should be torch.tensor on cuda
</span>    <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>                        
    <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># calc output from model 
</span>    <span class="k">if</span> <span class="n">target_label_idx</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">target_label_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">item</span><span class="p">()</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">output</span><span class="p">.</span><span class="nf">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">target_label_idx</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span> 
    <span class="k">if</span> <span class="n">cuda</span><span class="p">:</span>
      <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="nf">cuda</span><span class="p">()</span>                     <span class="c1"># calc prediction
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>           <span class="c1"># gather functionality of pytorch
</span>    <span class="k">return</span> <span class="n">target_label_idx</span><span class="p">,</span> <span class="n">output</span> 

<span class="nb">input</span> <span class="o">=</span> <span class="nf">pre_processing</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>          <span class="c1"># preprocess: image (normalise, transpose, make tensor on cuda, requires_grad=True)
</span><span class="nf">print </span><span class="p">(</span><span class="nb">input</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">label</span><span class="p">,</span> <span class="n">output</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">output:</span><span class="sh">'</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">output label:</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>The following code helps you to get the label of the prediction. The label is the index of the class in the ImageNet dataset. The index is used to get the class name from the <mark>.json</mark> file. The <mark>.json</mark> file is available in the repository.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="n">idx2label</span><span class="p">,</span> <span class="n">cls2label</span><span class="p">,</span> <span class="n">cls2idx</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">{},</span> <span class="p">{}</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">abspath</span><span class="p">(</span><span class="sh">'</span><span class="s">imagenet_class_index.json</span><span class="sh">'</span><span class="p">),</span> <span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">read_file</span><span class="p">:</span>
    <span class="n">class_idx</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">read_file</span><span class="p">)</span>
    <span class="n">idx2label</span> <span class="o">=</span> <span class="p">[</span><span class="n">class_idx</span><span class="p">[</span><span class="nf">str</span><span class="p">(</span><span class="n">k</span><span class="p">)][</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">class_idx</span><span class="p">))]</span>
    <span class="n">cls2label</span> <span class="o">=</span> <span class="p">{</span><span class="n">class_idx</span><span class="p">[</span><span class="nf">str</span><span class="p">(</span><span class="n">k</span><span class="p">)][</span><span class="mi">0</span><span class="p">]:</span> <span class="n">class_idx</span><span class="p">[</span><span class="nf">str</span><span class="p">(</span><span class="n">k</span><span class="p">)][</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">class_idx</span><span class="p">))}</span>
    <span class="n">cls2idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">class_idx</span><span class="p">[</span><span class="nf">str</span><span class="p">(</span><span class="n">k</span><span class="p">)][</span><span class="mi">0</span><span class="p">]:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">class_idx</span><span class="p">))}</span> 
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="lime-explanation">LIME explanation</h2>
<p>The following figure illustrates the basic idea behind LIME. The figure
shows light pink and blue areas which are the decision boundaries for
the classifier (for the VGG19 pre-trained model on ImageNet for
instance). <mark style="background-color:Lavender;">LIME</mark> can
provide explanations for the predictions of an individual instance (the
one with the dark red dot). These explanations are created by generating
a new dataset of perturbations around the instance to be explained (in
our image are depicted with dot circles around the initial instance).</p>

<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2023-05-13-LIME/LIME.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>Then, we apply our
<mark style="background-color:Lavender;">black-box</mark> model
<mark style="background-color:Lavender;">$g()$</mark> and we can extract
the label for all the perturbations (and can be seen with red that
denotes turtle and purple that denotes non-turtle). The
<em>importance</em> of each perturbation is determined by measuring its
distance from the original instance to be explained
(<mark style="background-color:Lavender;">. These distances are
converted to weights by mapping the distances to a zero-one scale using
a kernel function ($\pi_{\mathbf{x}}$ </mark>). All this information:
the newly generated dataset, its class predictions and its weights are
used to fit a simple model, such as a linear model (gray line), that can
be interpreted. The coefficients for this model are extracted and used
as the explanation for the prediction of the instance we want to
explain. The higher the coefficient, the more important the feature is
for the prediction.</p>

<h2 id="creating-perturbations-of-image">Creating Perturbations of image</h2>

<p>In the case of image explanations, our perturbations will be generated
by zeroing out some of the superpixels in the image.</p>

<h4 id="extract-super-pixels-from-the-image">Extract super-pixels from the image</h4>

<p>The superpixels are generated using the
<mark style="background-color:Lavender;">quickshift</mark> segmentation
algorithm. This algorithm is provided by the
<mark style="background-color:Lavender;">skimage.segmentation</mark>
library. It can be noted that for the given image,
<mark style="background-color:Lavender;">68</mark> superpixels were
generated. That of course will change if you will update the
<mark style="background-color:Lavender;">quickshift</mark> parameters
(<mark style="background-color:Lavender;">kernel_size</mark> and
<mark style="background-color:Lavender;">max_dist</mark>). The code
snippet can be found below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">skimage.io</span> 
<span class="kn">import</span> <span class="n">skimage.segmentation</span>

<span class="n">superpixels</span> <span class="o">=</span> <span class="n">skimage</span><span class="p">.</span><span class="n">segmentation</span><span class="p">.</span><span class="nf">quickshift</span><span class="p">(</span><span class="n">img</span><span class="o">/</span><span class="mi">255</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">max_dist</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">ratio</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">num_superpixels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">superpixels</span><span class="p">).</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">num_superpixels</span>

<span class="n">imgplot</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">skimage</span><span class="p">.</span><span class="n">segmentation</span><span class="p">.</span><span class="nf">mark_boundaries</span><span class="p">(</span><span class="n">img</span><span class="o">/</span><span class="mi">255</span><span class="p">,</span> <span class="n">superpixels</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</pre></td></tr></tbody></table></code></pre></div></div>

<p>The generated superpixels for the input Caretta-caretta image are shown
in the image below. Note that you can improve the result of the approach
or use another method for creating the superpixels.</p>

<p><img src="../../../assets/img/2023-05-13-LIME/caretta_2.jpg" style="width:400px;height:400px;" class="center" /></p>

<h3 id="creating-random-perturbations">Creating random perturbations</h3>
<p>In this example, 14 super-pixels were used. However, for real-life
applications, a larger number of super-pixels will produce more
reliable explanations.</p>

<p>Having extracted the super-pixels, the way that perturbations are calculated is the following: Random
zeros and ones are generated and shaped as a matrix with perturbations
as rows and superpixels as columns. An example of a perturbation (the
first one) is shown below. Here, <code class="language-plaintext highlighter-rouge">1</code> represents that a superpixel is on
and <code class="language-plaintext highlighter-rouge">0</code> represents it is off. Notice that the length of the shown vector
corresponds to the number of superpixels in the image. By running this
code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="n">num_perturb</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">perturbations</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_perturb</span><span class="p">,</span> <span class="n">num_superpixels</span><span class="p">))</span>
<span class="n">perturbations</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1">#Show example of perturbation
</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>We can produce this sequence:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre> <span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>The following function <code class="language-plaintext highlighter-rouge">perturb_image</code> perturbs the given image (<code class="language-plaintext highlighter-rouge">img</code>)
based on a perturbation vector (<code class="language-plaintext highlighter-rouge">perturbation</code>) and predefined
superpixels (<code class="language-plaintext highlighter-rouge">segments</code>).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">perturb_image</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">perturbation</span><span class="p">,</span><span class="n">segments</span><span class="p">):</span>
  <span class="n">active_pixels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">perturbation</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">segments</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="c1">#Add your code here
</span>  <span class="k">return</span> <span class="n">perturbed_image</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Let’s use the previous function to see what a perturbed image would look like:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">skimage</span><span class="p">.</span><span class="n">io</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="nf">perturb_image</span><span class="p">(</span><span class="n">img</span><span class="o">/</span><span class="mi">255</span><span class="p">,</span><span class="n">perturbations</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">superpixels</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="../../../assets/img/2023-05-13-LIME/caretta_3.jpg" style="width:400px;height:400px;" class="center" /></p>

<h3 id="step-2-use-ml-classifier-to-predict-classes-of-newly-generated-images">Step 2: Use ML classifier to predict classes of newly generated images</h3>

<p>This is the most computationally expensive step in LIME because a
prediction for each perturbed image is computed. Our aim is to construct a label for each pertubation. From the shape of the
predictions, we can see for each of the perturbations we have the output
probability for each of the 1000 classes.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="rouge-code"><pre><span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">pert</span> <span class="ow">in</span> <span class="n">perturbations</span><span class="p">:</span>
  <span class="n">perturbed_img</span> <span class="o">=</span> <span class="nf">perturb_image</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">pert</span><span class="p">,</span><span class="n">superpixels</span><span class="p">)</span>
  <span class="nb">input</span> <span class="o">=</span> <span class="nf">pre_processing</span><span class="p">(</span><span class="n">perturbed_img</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>   
  <span class="c1"># preprocess: image (normalise, transpose, make tensor on cuda, requires_grad=True)
</span>  <span class="n">output</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>
  
  <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>                        
  <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="nf">print </span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">target_label_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">item</span><span class="p">()</span>
  
  <span class="n">predictions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
<span class="n">predictions</span><span class="p">.</span><span class="n">shape</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Then, we will need to calculate the distances between the perturbations
and the original image. The distances are calculated using the cosine
distance. The smaller the distance, the more similar the vectors are.
The distances are then converted to weights using a kernel function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="n">original_image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">num_superpixels</span><span class="p">)[</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">,:]</span> <span class="c1">#Perturbation with all superpixels enabled 
</span><span class="n">distances</span> <span class="o">=</span> <span class="n">sklearn</span><span class="p">.</span><span class="n">metrics</span><span class="p">.</span><span class="nf">pairwise_distances</span><span class="p">(</span><span class="n">perturbations</span><span class="p">,</span><span class="n">original_image</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">cosine</span><span class="sh">'</span><span class="p">).</span><span class="nf">ravel</span><span class="p">()</span>
<span class="n">distances</span><span class="p">.</span><span class="n">shape</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>As we have shown before, after calculating the distances we need to apply
kernels to use them in the loss function. The employed can is as
follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="n">kernel_width</span> <span class="o">=</span> <span class="mf">0.25</span>	
<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">distances</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">kernel_width</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="c1">#Kernel function 
</span><span class="n">weights</span><span class="p">.</span><span class="n">shape</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Having calculated the proximity of each perturbation from the original image, as a final step we can calculate the explanation for our predictions. To do that, we will train the explainer using the generated perturbations. Finally, we will need to keep the five highest values in our calculated weights.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="rouge-code"><pre><span class="n">out</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>     
<span class="nf">print </span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>                
<span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>    

<span class="n">out</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">top_values</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">5</span><span class="p">]</span> <span class="c1"># Keep the first 5 values from each row
</span><span class="n">top_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">5</span><span class="p">]</span>   <span class="c1"># Keep the corresponding indices
</span>
<span class="n">top5</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">topk</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">topk_values</span> <span class="o">=</span> <span class="n">top_values</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">topk_indices</span> <span class="o">=</span>  <span class="n">top_indices</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="n">topk_values</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">topk_indices</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>We can now train the linear model using the perturbations and the
predictions with the highest probability. The extracted coefficients (or
the explainer model weights) of the linear model are the explanations
for the predictions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="n">simpler_model</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>
<span class="c1">#simpler_model.fit()
</span><span class="n">coeff</span> <span class="o">=</span> <span class="n">simpler_model</span><span class="p">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">coeff</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>We can now visualise the top features that the classifier used to make its prediction. The top features are the superpixels that have the highest coefficients.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="n">num_top_features</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">top_features</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">coeff</span><span class="p">)[</span><span class="o">-</span><span class="n">num_top_features</span><span class="p">:]</span> 
<span class="n">top_features</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Having calculated the top features, we can now create a mask that will highlight the top superpixels in the image.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">num_superpixels</span><span class="p">)</span> 
<span class="n">mask</span><span class="p">[</span><span class="n">top_features</span><span class="p">]</span><span class="o">=</span> <span class="bp">True</span> <span class="c1">#Activate top superpixels
</span><span class="n">skimage</span><span class="p">.</span><span class="n">io</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="nf">perturb_image</span><span class="p">(</span><span class="n">img</span> <span class="p">,</span><span class="n">mask</span><span class="p">,</span><span class="n">superpixels</span><span class="p">)</span> <span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="../../../assets/img/2023-05-13-LIME/caretta_4.jpg" style="width:400px;height:400px;" class="center" /></p>

<h1 id="lab-exercise-tosubmit">Lab exercise <mark>TOSUBMIT</mark></h1>

<p>This part of the tutorial is a lab exercise where you will use LIME to
debias a classifier. The goal is to understand how LIME can be used to
explain the behavior of a classifier and how we can use this information
to debias the classifier.</p>

<p>The classifier is trained to classify images of <em>huskies</em> and
<em>wolves</em>. The classifier is a
<mark style="background-color:Lavender;"> black-box $f(.)$</mark> so we
do not have access to its internal states. We have zero knowledge of the training dataset
but also of the classifier’s architecture.</p>

<p>We hope that we will be able to identify the biases in the classifier and
try to debias it using LIME explainer.</p>

<p>We will need to explain the behavior of the classifier
using images from huskies and wolves. You should make use also the
images that you can find on the Canvas page of the course or from google
colab page. If you want you can use also your own husky and wolf images.</p>

<p>Firstly, we will load the classifier and the images we want to explain.
The following code checks the performance of the classifier in a test
set.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">check_accuracy</span><span class="p">(</span><span class="n">loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">num_correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            
            <span class="n">scores</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">predictions</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            
            <span class="nf">print </span><span class="p">(</span><span class="sh">"</span><span class="s">prediction </span><span class="sh">"</span><span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span> <span class="o">+</span> <span class="sh">"</span><span class="s"> label </span><span class="sh">"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
            <span class="n">num_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">y</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>
            <span class="n">num_samples</span> <span class="o">+=</span> <span class="n">predictions</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Got </span><span class="si">{</span><span class="n">num_correct</span><span class="si">}</span><span class="s"> / </span><span class="si">{</span><span class="n">num_samples</span><span class="si">}</span><span class="s"> with accuracy </span><span class="si">{</span><span class="nf">float</span><span class="p">(</span><span class="n">num_correct</span><span class="p">)</span><span class="o">/</span><span class="nf">float</span><span class="p">(</span><span class="n">num_samples</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span> 
    
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Now we will load the classifier, load the images and check the performance of the classifier.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="sh">'</span><span class="s">model/hus_wolf_model.pth</span><span class="sh">'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>The following code loads the images and creates the dataloader. The
dataloader is used to load the images in batches. The images are
preprocessed using the same transformations that were used to train the
classifier. Note that the following code is the same as the one used in
the previous part of the tutorial for preprocessing images.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="rouge-code"><pre><span class="c1"># create the transformation for all the images
</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="p">.</span><span class="nc">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
        <span class="n">transforms</span><span class="p">.</span><span class="nc">RandomCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
        <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span><span class="mf">0.225</span><span class="p">])])</span>

<span class="c1"># Load the images
</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">ImageFolder</span><span class="p">(</span><span class="sh">"</span><span class="s">data2/test/</span><span class="sh">"</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>

<span class="n">dataset</span><span class="p">.</span><span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">husky</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">wolf</span><span class="sh">'</span><span class="p">]</span>
<span class="n">dataset</span><span class="p">.</span><span class="n">class_to_idx</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">husky</span><span class="sh">'</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span><span class="sh">'</span><span class="s">wolf</span><span class="sh">'</span><span class="p">:</span><span class="mi">1</span><span class="p">}</span>
<span class="n">dataset</span><span class="p">.</span><span class="n">samples</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">dataset</span><span class="p">.</span><span class="n">samples</span><span class="p">))</span>
<span class="n">test_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>

<span class="c1"># Check the accuracy of the classifier
</span><span class="nf">check_accuracy</span><span class="p">(</span><span class="n">test_dataloader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>The expected result should be around <mark style="background-color:Lavender;">36%</mark> accuracy.</p>

<p>As a <em>lab exercise</em>, and by using the LIME algorithm established
in previous cells, you should explain the behavior of the classifier.
You should use the provided images in <mark>data/test/</mark> folder.
<mark style="background-color:Lavender;"> Add your conclusions about the
model $f()$</mark>. What are the main problems with this classifier?
Let’s discuss the results during the Werkcollege.</p>

<h1 id="you-should-not-trust-me-tosubmit">You should not trust me! <mark>TOSUBMIT</mark></h1>

<p>Now you feel like a <em>hacker</em> and you would like to fool LIME
explanations by constructing adversarial classifiers. To do that we will
follow the process that is described in the following paper [4]. You
can find a detailed also explanation in the following video:</p>

<p><a href="https://www.youtube.com/watch?v=qCYAKmFFpbs&amp;ab_channel=MLExplained-AggregateIntellect-AI.SCIENCE" title="You should not trust me!"><img src="../../../assets/img/2023-05-13-LIME/fooling_LIME.PNG" alt="You should not trust me!" /></a></p>

<p>This part of the tutorial is the third part of the assignment you need to
  <mark>TOSUBMIT</mark>. The goal is to understand the limitations of LIME
  and how we can take advantage of them to perform adversarial attacks.
  The main focus is to understand the way that LIME generates samples (the
  perturbations and the distribution of the perturbations in comparison
  with the real data).</p>

<p>The setup is the following:</p>

<p>As an adversary hacker, you intend to deploy the biased
  classifier $f$ for making a <em>critical decision</em> (e.g., confusing
  huskies with wolves 😱) in the real world. The adversary must provide
  black-box access to customers and examiners, who may use post hoc
  explanation techniques to better understand $f$ and determine if $f$ is
  ready to be used in the real world. If customers and regulators detect
  that $f$ is biased, they are not likely to approve it for deployment.
  The goal of the adversary is to fool post hoc explanation techniques and
  hide the underlying biases of $f$.</p>

<p>In this assignment, check you need to illustrate the difference between the distribution of real samples and the perturbations. Find a way to illustrate that in an example of your choice.</p>

<p><mark>Optionally</mark>, you should create a scaffolded classifier $e$ that
  behaves exactly like $f$ when making predictions on instances sampled
  from $X_{dist}$ (real samples) but will not reveal the underlying biases of $f$ when probed with leading post hoc explanation techniques such as LIME.</p>

<p>To do that you will need to follow the steps of the suggested paper. You
  need to find ways to differentiate the real data from the generated data
  and based on that create the scaffolded classifier $e$ that will fool
  LIME.</p>

<p>For this assignment, you will need to read the proposed paper and think of ways to perform adversarial attacks using the provided model. Add in your report an explanation of the procedure and the image with the difference between the 2 distributions. Optionally you can add examples of how the classifier $e$ can work in our case.</p>

<h2 id="guidelines-for-the-canvas-submissions">Guidelines for the Canvas submissions:</h2>

<ul>
  <li>You should prepare a report based on researching the <mark>TOSUBMIT</mark> from both notebooks.</li>
  <li>This report should contain three paragraphs. One will be about Integrated gradient and the questions that can be found at the end of the first notebook, one for the LIME explanations and finally one about adversarial attacks.</li>
  <li>Optionally, you should add one paragraph with the results of a scaffolding classifier that can fool LIME and some visual results with that.</li>
</ul>

<h1 id="journal-club-for-weeks-2-and-3">Journal club for weeks 2 and 3</h1>

<p>The papers for weeks 2 and 3 can be found on Canvas at the following links:</p>

<ul>
  <li>https://arxiv.org/pdf/1711.11279.pdf</li>
  <li>https://openaccess.thecvf.com/content/CVPR2021/papers/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.pdf</li>
</ul>

<h1 id="mini-project-ideas">Mini-project ideas</h1>

<p>This part of the tutorial relates to ideas about extending parts of this tutorial. One point of criticism of the LIME technique is the way that the perturbations are taking place since the distributions between real and generated data are significantly different. Think of ways to improve the gap (using alternative ways to perform perturbations).</p>
<ul>
  <li>Think of ways to improve the feature repressions (replacing super-pixels).</li>
  <li>Another direction could be to come up with an evaluation scheme and perform a study comparing LIME, IG or other methods for explainable AI.</li>
</ul>

<h1 id="conclusions">Conclusions</h1>

<p>In this tutorial, we have analyzed LIME a posthoc XAI technique. An explanation of how this technique works but also step-by-step the code to implement it. We have also seen how we can use LIME to explain image classifiers but also how to identify the bias in a classifier.</p>

<h1 id="references">References</h1>
<p style="font-size: smaller"><a href="https://arxiv.org/pdf/1602.04938.pdf">[1] Ribeuro, M.T. et al. Why Should I Trust You? Explaining the Predictions of Any Classifier, 2016. SIGKDD.</a></p>

<p style="font-size: smaller"><a href="https://arxiv.org/pdf/2205.11487">[2] Slack, D. et al. Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods, 2020, AIES ‘20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</a></p>

<p style="font-size: smaller"><a href="https://arxiv.org/pdf/2205.11487">[3] Slack, D. et al. Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods, 2020, AIES ‘20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</a></p>

<p style="font-size: smaller"><a href="https://arxiv.org/pdf/2205.11487">[4] Slack, D. et al. Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods, 2020, AIES ‘20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</a></p>]]></content><author><name>Christos Athanasiadis</name></author><summary type="html"><![CDATA[This page's goal is to present a PostHoc feature attribution XAI methodology called LIME (Local Interpretable Model-agnostic Explanations) and demonstrate how it can be used to explain image classification tasks. You will be guided through the code and the results of the LIME algorithm. Part of this workshop will be some research questions that need to be answered by you. These questions can be found in the last part of the tutorials and you will encounter these questions with the TOSUBMIT tag. You will need to submit your answers to these questions in the form of a .pdf file on Canvas. The deadline for submitting your answers is the .. of June 2023.]]></summary></entry><entry><title type="html">Gradient-based feature attribution</title><link href="https://kristosh.github.io//blog/2023/gradient_based_feature_attribution/" rel="alternate" type="text/html" title="Gradient-based feature attribution" /><published>2023-05-13T00:00:00+00:00</published><updated>2023-05-13T00:00:00+00:00</updated><id>https://kristosh.github.io//blog/2023/gradient_based_feature_attribution</id><content type="html" xml:base="https://kristosh.github.io//blog/2023/gradient_based_feature_attribution/"><![CDATA[<h1 id="gradient-based-feature-attribution-for-vision">Gradient-based feature attribution for Vision</h1>

<p>Before starting with the explanation of the gradient-based methodologies, we provide some useful code for all the necessarry packages loacing a pre-trained VGG model (in Imagenet) but also code to load a image for a local directory in PyTorch. You can access the code of this tutorial in the following <a href="https://colab.research.google.com/drive/1zWmtpOTXfxv1Hxwl7G5heU73vz90iNhl?usp=sharing">google colab link</a>.</p>

<p>The packages that you will need to import are the following ones:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre><span class="c1"># set-up environment
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">models</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="n">cv2</span>
<span class="kn">import</span> <span class="n">os</span>  <span class="c1"># necessary
</span><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="c1">#from google.colab.patches import cv2_imshow   # specific for colab
</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">KMP_DUPLICATE_LIB_OK</span><span class="sh">'</span><span class="p">]</span><span class="o">=</span><span class="sh">'</span><span class="s">True</span><span class="sh">'</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="load-a-pretrained-model">Load a pretrained model</h3>

<p>We will make use of the VGG19 CNN network and ImageNet.</p>

<ul>
  <li>ImageNet is a large collection of images.</li>
  <li>VGG19 is a convolutional neural network architecture.</li>
  <li>
    <p>We can load a version that is trained on ImageNet and that can detect objects in 1000 classes.</p>
  </li>
  <li>Read about and understand VGG ConvNet and Imagenet for background.</li>
</ul>

<p>The first step is that using the pytorch library, we load the pretrained version of VGG19.</p>

<p>Since we will not train the model we set the model in evaluation mode.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="c1"># load model
# model_type = 'vgg19'
</span><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nf">vgg19</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># run it on a GPU if available:
</span><span class="n">cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda:</span><span class="sh">'</span><span class="p">,</span> <span class="n">cuda</span><span class="p">,</span> <span class="sh">'</span><span class="s">device:</span><span class="sh">'</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># set model to evaluation
</span><span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>The output should look like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
</pre></td><td class="rouge-code"><pre><span class="n">Output</span> <span class="n">exceeds</span> <span class="n">the</span> <span class="n">size</span> <span class="n">limit</span><span class="p">.</span> <span class="n">Open</span> <span class="n">the</span> <span class="n">full</span> <span class="n">output</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">text</span> <span class="nf">editorVGG</span><span class="p">(</span>
  <span class="p">(</span><span class="n">features</span><span class="p">):</span> <span class="nc">Sequential</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">4</span><span class="p">):</span> <span class="nc">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">5</span><span class="p">):</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">6</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">7</span><span class="p">):</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">8</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">9</span><span class="p">):</span> <span class="nc">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">10</span><span class="p">):</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">11</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">12</span><span class="p">):</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">13</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">14</span><span class="p">):</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">15</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">16</span><span class="p">):</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">17</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">18</span><span class="p">):</span> <span class="nc">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">19</span><span class="p">):</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">20</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">21</span><span class="p">):</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">22</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="bp">...</span>
    <span class="p">(</span><span class="mi">4</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">5</span><span class="p">):</span> <span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">6</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  <span class="p">)</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="load-and-preprocess-the-images">Load and preprocess the images:</h3>

<p>We have provided a few images of wildlife, but please also use you own imagery. Set the path to your data-file and load an image.</p>

<p>VGG-19 works best if image is normalised. Image should also be in the correct tensor format.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">pre_processing</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">cuda</span><span class="p">):</span>
    <span class="c1"># Students should transpose the image to the correct tensor format. 
</span>    <span class="c1"># Students should ensure that gradient for input is calculated       
</span>    <span class="c1"># set the GPU device
</span>    <span class="k">if</span> <span class="n">cuda</span><span class="p">:</span>
        <span class="n">torch_device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda:0</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">torch_device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># normalise for ImageNet
</span>    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">]).</span><span class="nf">reshape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]).</span><span class="nf">reshape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span> <span class="o">/</span> <span class="mi">255</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="p">(</span><span class="n">obs</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span>

    <span class="c1"># make tensor format that keeps track of gradient
</span>    <span class="c1"># BEGIN for students to do
</span>    <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>       
    <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
    <span class="n">obs_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch_device</span><span class="p">)</span>
    <span class="c1"># END for students to do
</span>    <span class="k">return</span> <span class="n">obs_tensor</span>

</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="rouge-code"><pre><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="c1">#The line above is necesary to show Matplotlib's plots inside a Jupyter Notebook
</span><span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># read the image and convert it - Set your pathto the image
</span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">imread</span><span class="p">(</span><span class="sh">'</span><span class="s">elephant-zebra.png</span><span class="sh">'</span><span class="p">)</span>
<span class="c1">#img = cv2.imread(datafiles+ 'R.png')
#img = cv2.imread(datafiles+ 'elephant/Elephant2.jpeg')
# img = cv2.imread(datafiles+ 'shark/Shark1.jpeg')
</span><span class="nf">if </span><span class="p">(</span><span class="nf">type</span><span class="p">(</span><span class="n">img</span><span class="p">)</span> <span class="ow">is</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
  <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">resize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
  <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span>
  <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">img:</span><span class="sh">'</span><span class="p">,</span> <span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>


<span class="k">else</span><span class="p">:</span>
  <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">image not found - set your path to the image</span><span class="sh">'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">white</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nc">Axes</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_axis_off</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">add_axes</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="o">/</span><span class="mi">255</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="predict-class">Predict class</h3>

<p>We can easily predict the class, and the softmax score of that prediction:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">target_label_idx</span><span class="p">,</span> <span class="n">cuda</span><span class="p">):</span>
    <span class="c1"># Makes prediction after preprocessing image 
</span>    <span class="c1"># Note that output should be torch.tensor on cuda
</span>    <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>                        
    <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># calc output from model 
</span>    <span class="k">if</span> <span class="n">target_label_idx</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">target_label_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">item</span><span class="p">()</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">output</span><span class="p">.</span><span class="nf">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">target_label_idx</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span> 
    <span class="k">if</span> <span class="n">cuda</span><span class="p">:</span>
      <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="nf">cuda</span><span class="p">()</span>                     <span class="c1"># calc prediction
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>           <span class="c1"># gather functionality of pytorch
</span>    <span class="k">return</span> <span class="n">target_label_idx</span><span class="p">,</span> <span class="n">output</span> 

<span class="c1"># test preprocessing
# you can check that the VGG network gives a correct prediction. E.g. 385 and 386 are 'Indian Elephant'and 'African Elephant'
</span><span class="nb">input</span> <span class="o">=</span> <span class="nf">pre_processing</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>          <span class="c1"># preprocess: image (normalise, transpose, make tensor on cuda, requires_grad=True)
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">output:</span><span class="sh">'</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>TODO 1</strong></p>
<ul>
  <li>Upload at least two images to your directory with the first one containing a single animal and the second image with two animals.</li>
  <li>Run the classifier with both images.</li>
  <li>Look up the predicted categories and the ImageNet labels.</li>
  <li>Look up the indices corresponsing to each of the animals in your images.</li>
</ul>

<p>The following snippets might be useful:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">raw</span><span class="p">.</span><span class="n">githubusercontent</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">hub</span><span class="o">/</span><span class="n">master</span><span class="o">/</span><span class="n">imagenet_classes</span><span class="p">.</span><span class="n">txt</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">imagenet_classes.txt</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">categories</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">f</span><span class="p">.</span><span class="nf">readlines</span><span class="p">()]</span>
<span class="nf">print</span><span class="p">(</span><span class="n">categories</span><span class="p">[</span><span class="mi">385</span><span class="p">])</span>
<span class="n">categories</span><span class="p">.</span><span class="nf">index</span><span class="p">(</span><span class="sh">'</span><span class="s">Indian elephant</span><span class="sh">'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="compute-the-gradient-with-respect-to-the-input-pixels">Compute the gradient with respect to the input pixels</h3>

<p>Now that we can predict the class of an object, we will try to understand what image pixels are most important for the prediction using <em>feature attribution methods</em>. The first technique that we will make use is the saliency maps. In short this approach determines the gradient of the output w.r.t to the input.</p>

<p>The idea of Saliency maps (called <em> Vanilla Gradient </em> as well), introduced by Simonyan et al. (https://arxiv.org/pdf/1312.6034.pdf) as one of the first pixel attribution approaches. The core idea is really simple and what needs to be done is to calculate the gradient of the loss function for the class we are interested in with respect to the input features. This gives us a map of the size of the input features with negative to positive values.</p>

<p>The recipe for this approach is as follows:</p>

<ul>
  <li><strong>Perform a forward pass</strong> of the image ($\mathbf{x}_0$) of interest using the network $\mathcal{F}(\mathbf{x}_0)$.</li>
  <li><strong>Compute the gradient</strong> of class score of interest with respect to the input image ($\mathbf{x}_0$): $g(\mathbf{x}_0) = \frac{\partial \mathcal{F}}{\partial \mathbf{x}_0} $.</li>
  <li><strong>Visualize the gradients</strong>: You can either show the absolute values or highlight negative and positive contributions separately.</li>
</ul>

<h3 id="the-instructions-for-the-pytorch-code">The instructions for the PyTorch code:</h3>

<p>We have set the model in eval mode, but we can still catch the gradients of the input-image if ask PyTorch to do this and then do some backward calculation. That is what you need to do. So complete the procedure in order that:</p>
<ul>
  <li>Input should be preprocessed (and converted into a torch tensor).</li>
  <li>Set the <mark>required_gradient=True</mark> on the input tensor.</li>
  <li>Calculate the output (with previous method predict).</li>
  <li>Set the gradient to zero and do a backward on the output.</li>
  <li>Gradients w.r.t input can now be found under input.grad</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">calculate_outputs_and_gradients</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">target_label_idx</span><span class="p">,</span> <span class="n">cuda</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="c1"># Calculates the gradient of the output w.r.t. the input image
</span>    <span class="c1"># The result should be a gradients numpy matrix of same dimensions as the inputs
</span>    <span class="n">predict_idx</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>                             <span class="c1"># for every image
</span>        <span class="nb">input</span> <span class="o">=</span> <span class="nf">pre_processing</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>  
        <span class="nb">input</span><span class="p">.</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span>
        <span class="nf">print </span><span class="p">(</span><span class="n">target_label_idx</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">output</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">target_label_idx</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>
        <span class="c1"># clear grad
</span>        <span class="c1">## BEGIN student code
</span>        <span class="c1"># Perform a backward pass on the output and collect the gradient w.r.t. the input
</span>        <span class="c1"># Store this gradient in the variable 'gradient' 
</span>        <span class="c1">## END student code
</span>        <span class="n">gradient</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># do backward and gather gradients of input
</span>        
        <span class="n">gradients</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gradients</span><span class="p">,</span> <span class="n">target_label_idx</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="c1"># calculate the gradient and the label index
#gradients, label_index = calculate_outputs_and_gradients([img], model, None, cuda)    
</span><span class="n">gradients</span><span class="p">,</span> <span class="n">label_index</span> <span class="o">=</span> <span class="nf">calculate_outputs_and_gradients</span><span class="p">([</span><span class="n">img</span><span class="p">],</span> <span class="n">model</span><span class="p">,</span> <span class="n">target_label_idx</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">cuda</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>    
<span class="n">gradients</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">gradients</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

<span class="c1">#Note that if target_label_idx == None, the calculate_output)and_gradients function assume:
#            target_label_idx = torch.argmax(output, 1).item()
</span>
<span class="c1"># Please note that the dimensions of gradients are same as dimensions of input
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">gradients</span><span class="sh">'</span><span class="p">,</span> <span class="n">gradients</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> 
<span class="c1"># Please note that gradients are positive and negative values
</span><span class="nf">print</span><span class="p">(</span><span class="n">gradients</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="p">:</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nf">print</span><span class="p">(</span><span class="n">label_index</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<font color="green"><b>ToDo 2</b></font>
<p>For your image with two animals, consider both ID’s as target_label_idx.</p>
<ul>
  <li>Run the classifier with each ID</li>
  <li>After running the forward pass, compute the gradients</li>
</ul>

<font color="blue"><b>ToThink 2</b></font>
<p>Are the gradients the same when you use different target classes? Why?</p>

<h3 id="visualize-the-gradients">Visualize the gradients</h3>

<p>Try to visualise the image and the saliency map.</p>

<p><strong>Tip:</strong> take absolute values of the gradients and maximize over all three channels.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="rouge-code"><pre>
<span class="c1"># Retrieve the saliency map and also pick the maximum value from channels on each pixel.
# In this case, we look at dim=2. Recall the shape of gradients (width, height, channel)
</span>
<span class="k">def</span> <span class="nf">plot_gradients</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">gradients</span><span class="p">,</span> <span class="n">title</span><span class="p">):</span>
  <span class="c1"># plots image (dimensions: Width X Heigth X 3) and gradients (dimensions: Width X Heigh x 3) - both numpy arrays
</span>  <span class="n">saliency</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">gradients</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>       <span class="c1"># takes maximum over 3 color channels                                                 
</span>  <span class="c1"># Visualize the image and the saliency map
</span>  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="o">/</span><span class="mi">255</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">imshow</span><span class="p">(</span><span class="n">saliency</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">hot</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
  <span class="n">fig</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="nf">plot_gradients</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">gradients</span><span class="p">,</span> <span class="sh">'</span><span class="s">The Image and Its Saliency Map</span><span class="sh">'</span><span class="p">)</span>

</pre></td></tr></tbody></table></code></pre></div></div>

<font color="red"><b>ToSubmit 1</b></font>
<p>In your report, include your two images, the two saliency map, the two target  label, the predicted label and the likelihood your models assigns to each label. Add a caption explaining very briefly (1 or 2 sentences) whether there’s a difference and why.</p>

<h3 id="issues-with-saliency-maps-and-vanilla-gradients-saturation">Issues with saliency maps and vanilla gradients (saturation)</h3>

<p>Vanilla Gradient methods, notoriously, are facing saturation problems, as explained in Avanti et al. (2017). When the ReLU is used, and when the activation goes below zero, then, the activation is limited at zero and does not change any more. Hence, the activation is saturated.</p>

<p>Therefore, multiple strategies have been proposed to deal with that issue. One of them is Gradient-weighted Class Activation Map (<em>Grad-Cam</em>) that instead of calculating the gradient to the input image it make use of the last convolutional layer.</p>

<h3 id="gradient-weighted-class-activation-mapping-grad-cam">Gradient-weighted Class Activation Mapping (Grad-CAM)</h3>

<h1 id="4-gradient-weighted-class-activation-mapping-grad-cam"><strong>4 Gradient-weighted Class Activation Mapping (Grad-Cam).</strong></h1>

<p>Unlike saliency maps, in the <strong>Grad-Cam</strong> approach the gradient is not backpropagated all the way back to the image, but (usually) to the last convolutional layer in order to generate a visualization map that highlights important regions of the input.</p>

<p>A naive visualization approach could be the following:</p>

<ul>
  <li>Simply employ the values for each feature map, (of the last convolutional layer),</li>
  <li>Then, average these feature maps and overlay this over our image (by rescaling back to initial size).</li>
</ul>

<p>However, while simple, it is not really helpful approach, since these maps encode information for all classes, while we are interested in a specific class. <strong>Grad-CAM</strong> needs to figure out the importance for each of the $k$ feature map $A_k \in \mathbb{R}^{w \times h}$ ($w$ the width and $h$ the height of the features maps) in respect to our class $c$ of interest.</p>

<p>We have to weight each pixel of each feature map with the gradient before averaging over the feature maps $A_k$. This heatmap is send through the ReLU function which set all negative values to zero. The reason for that is that we are only interested in the parts that contribute to the selected class $c$ and not to other classes. The final feature map is rescaled back to the original image size. We then overlay it over the original image for producing the final visualization.</p>

<p><strong>Grad Cam recipe:</strong></p>

<ul>
  <li>Forward-propagate the input image $\mathbf{x}_0$ through the convolutional VGG19 network by calculating the $\mathcal{F}(\mathbf{x}_0)$.</li>
  <li>Obtain the score for the class of interest, that means the activation before the softmax layer.</li>
  <li>All the rest classes’ activations should be set to zero.</li>
  <li>Back-propagate the gradient of the class of interest to the last convolutional layer before the fully connected layers:</li>
</ul>

\[\frac{\partial y_{c}}{\partial A^k}\]

<ul>
  <li>Weight each feature map “pixel” by the gradient for the class. Indices $i$ and $j$ refer to the width and height dimensions:</li>
</ul>

\[\alpha^{c}_{k} = \overbrace{\frac{1}{Z} \sum_i \sum_j}^{\text{global averaging pooling}} \underbrace{\frac{\partial y_{c}}{\partial A^{k}_{ij}}}_{\text{gradients of the backpropagation}}\]

<p>This means that the gradients are globally pooled.</p>

<ul>
  <li>Calculate an average of the feature maps, weighted per pixel by backpropagated gradient.</li>
  <li>Apply ReLU to the averaged feature map.</li>
</ul>

\[L_{ij}^c = ReLU \sum_k \alpha^{c}_{k} A^{k}_{ij}\]

<p>We now have a heatmap $L^c$ for the class $c$.</p>

<ul>
  <li>Regarding the visualization: Scale values of the $L^c$ to the interval between 0 and 1. Upscale the image and overlay it over the original image.</li>
</ul>

<p>In our classification example this approach uses the activation map of the final convolutional layer (with VGG: the final features layer). Note that such an Activation Map can be a block of $14 \times 14 \times 512$, where the $14 \times 14$ indicated a grid on the image (noted by subscripts i and j) and the 512 is the number of channels (features, noted by the letter k). <strong>Grad Cam</strong> pools the Activation Map over the channels, and it gives a weight equal to the contribution of each channel to the prediction. This contribution of each channel is calculated by taking the gradient of the output w.r.t. the Activation Map and then pool this over the spacial ($14\times14$) dimensions.</p>

<p>For the calculation of the gradient w.r.t the Activation Map we need a little PyTorch trick since this gradient cannot be accessed by default. The PyTorch trick is called a ‘hook’. We can register a hook on a tensor of the network. With a hook we can define a little program that is executed when the tensor is touched during a backward pass. In our case we register a hook on the Activation Map we want to study and that is the 36th layer of the VGG19 convolutional “features” layer. The hook needs to be registered during a forward pass, so we will redefine the forward pass for our model.</p>

<p>There is a nice youtube tutorial on pytorch and hooks https://www.youtube.com/watch?v=syLFCVYua6Q. (22 minutes but I think it is worth it)</p>

<h3 id="define-a-new-vgg-model-including-a-hook">Define a new VGG model including a hook</h3>

<p>The VGG() class is based on the pretrained models.vgg19 that we know now.</p>

<p>In the init, the Activation Map we want to study is defined. That is the output of the first 36 feature layers.</p>

<p>In the activations_hook method we define our hook that will store the gradient calculated on the tensor in self.gradients.</p>

<p>In the forward we execute all VGG layers ‘by hand’. The hook is registered on the output of the first 36 feature layers. And then the remaining layers are defined.</p>

<p>When defined, we load this model, move it to our GPU if available and put the model in eval mode.</p>

<h3 id="activity">Activity:</h3>
<p>Finish the code below by finishing the method get_activations_gradient.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">VGG</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="c1"># VGG class builds on vgg19 class. The extra feature is the registration of a hook, that
</span>    <span class="c1"># stores the gradient on the last convolutional vgg layer (vgg.features[:36] in self.gradient)
</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">VGG</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="c1"># get the pretrained VGG19 network
</span>        <span class="n">self</span><span class="p">.</span><span class="n">vgg</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nf">vgg19</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="c1"># self.vgg = model
</span>
        <span class="c1"># disect the network to access its last convolutional layer
</span>        <span class="n">self</span><span class="p">.</span><span class="n">features_conv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">vgg</span><span class="p">.</span><span class="n">features</span><span class="p">[:</span><span class="mi">36</span><span class="p">]</span>
        
        <span class="c1"># get the max pool of the features stem
</span>        <span class="n">self</span><span class="p">.</span><span class="n">max_pool</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        
        <span class="c1"># get the classifier of the vgg19
</span>        <span class="n">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">vgg</span><span class="p">.</span><span class="n">classifier</span>
        
        <span class="c1"># placeholder for the gradients
</span>        <span class="n">self</span><span class="p">.</span><span class="n">gradients</span> <span class="o">=</span> <span class="bp">None</span>
    
    <span class="c1"># hook for the gradients of the activations: it stores the calculated grad (on our tensor) in self.gradients.
</span>    <span class="k">def</span> <span class="nf">activations_hook</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gradients</span> <span class="o">=</span> <span class="n">grad</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># gives the output of the first 36 'feature' layers
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">features_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># register the hook (note: h is a handle, giving the hook a identifier, we do not use it here)
</span>        <span class="n">h</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">register_hook</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">activations_hook</span><span class="p">)</span>

        <span class="c1"># apply the remaining pooling
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">max_pool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">view</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># apply the remaining classifying
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>

    <span class="c1"># method for the gradient extraction
</span>    <span class="k">def</span> <span class="nf">get_activations_gradient</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1">## Should return the gradients of the output with respect to the last convolutional layer
</span>        <span class="c1">## BEGIN Students TODO
</span>        <span class="k">return</span> <span class="n">notImplemented</span>
        <span class="c1">## END students TODO
</span>    
    <span class="c1"># method for the activation exctraction
</span>    <span class="k">def</span> <span class="nf">get_activations</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1">## BEGIN Students TODO
</span>        <span class="k">return</span> <span class="n">notImplemented</span>
        <span class="c1">## END students TODO
</span>
<span class="n">vgg</span> <span class="o">=</span> <span class="nc">VGG</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda:</span><span class="sh">'</span><span class="p">,</span> <span class="n">cuda</span><span class="p">,</span> <span class="sh">'</span><span class="s">device:</span><span class="sh">'</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="n">vgg</span> <span class="o">=</span> <span class="n">vgg</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">vgg</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Now calculate the gradients of a prediction w.r.t. the activation map.**</p>

<p>For that we do a prediction with our newly defined model vgg, and perform a backward on the output (the logit of the prediction vector that is largest). After the backward, the gradients w.r.t the activation map are stored in self.gradient:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="c1"># get the most likely prediction of the model
</span>
<span class="nb">input</span> <span class="o">=</span> <span class="nf">pre_processing</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>          <span class="c1"># preprocess: image (normalise, transpose, make tensor on cuda, requires_grad=True)
</span><span class="nf">print</span><span class="p">(</span><span class="nb">input</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">pred</span><span class="p">,</span> <span class="n">output</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">vgg</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>                         
<span class="nf">print</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>                        

<span class="c1"># also with our newly created VGG(), you should find a correct class (2=shark, 385/386 = elephants)
</span></pre></td></tr></tbody></table></code></pre></div></div>
<p>And finally:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">output</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre></td><td class="rouge-code"><pre><span class="c1"># pull the gradients out of the model
</span><span class="n">gradients</span> <span class="o">=</span> <span class="n">vgg</span><span class="p">.</span><span class="nf">get_activations_gradient</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">gradients:</span><span class="sh">'</span><span class="p">,</span> <span class="n">gradients</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># pool the gradients across the channels
</span><span class="n">pooled_gradients</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">pooled gradients:</span><span class="sh">'</span><span class="p">,</span> <span class="n">pooled_gradients</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># get the activations of the last convolutional layer
</span><span class="n">activations</span> <span class="o">=</span> <span class="n">vgg</span><span class="p">.</span><span class="nf">get_activations</span><span class="p">(</span><span class="nb">input</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>

<span class="c1"># weight the channels by corresponding gradients
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">512</span><span class="p">):</span>
    <span class="n">activations</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">*=</span> <span class="n">pooled_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    
<span class="c1"># average the channels of the activations
</span><span class="n">heatmap</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">()</span>

<span class="c1"># relu on top of the heatmap
# expression (2) in https://arxiv.org/pdf/1610.02391.pdf
# heatmap = np.maximum(heatmap, 0)
</span><span class="n">heatmap</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="n">heatmap</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="c1"># normalize the heatmap
</span><span class="n">heatmap</span> <span class="o">/=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">heatmap</span><span class="p">)</span>
<span class="c1"># END students TODO
</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">heatmap:</span><span class="sh">'</span><span class="p">,</span> <span class="n">heatmap</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># draw the heatmap
</span><span class="n">heatmap</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">squeeze</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">matshow</span><span class="p">(</span><span class="n">heatmap</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<font color="green"><b>ToDo 3</b></font>
<p>For your image with 2 animals and each of the two target categories:</p>
<ul>
  <li>Perform the forward pass again, but now with our adapted VGG model</li>
  <li>Draw the Grad-CAM heatmaps $L^c$.</li>
</ul>

<p>Code snippet that might be useful:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>gradients, label_index = calculate_outputs_and_gradients([img], vgg, target_label_idx=None, cuda=True)    
gradients = np.transpose(gradients[0], (1, 2, 0))
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="overlaying-heatmaps-and-iamges">Overlaying heatmaps and iamges:</h3>
<p>Now we have the heatmap, we can overlay it on the original image.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="c1"># draw the image
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">img:</span><span class="sh">'</span><span class="p">,</span> <span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">matshow</span><span class="p">(</span><span class="n">img</span><span class="o">/</span><span class="mi">255</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre><span class="nf">print</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">heatmap</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">heatmap</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">resize</span><span class="p">(</span><span class="n">heatmap</span><span class="p">,</span> <span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">heatmap</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">uint8</span><span class="p">(</span><span class="mi">255</span> <span class="o">*</span> <span class="n">heatmap</span><span class="p">)</span>
<span class="n">heatmap</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">applyColorMap</span><span class="p">(</span><span class="n">heatmap</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLORMAP_JET</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">img</span><span class="p">.</span><span class="nf">min</span><span class="p">(),</span> <span class="n">img</span><span class="p">.</span><span class="nf">max</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">heatmap</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">heatmap</span><span class="p">.</span><span class="nf">min</span><span class="p">(),</span> <span class="n">heatmap</span><span class="p">.</span><span class="nf">max</span><span class="p">())</span>

<span class="n">super_img</span> <span class="o">=</span> <span class="n">heatmap</span> <span class="o">*</span> <span class="mf">0.4</span> <span class="o">+</span> <span class="n">img</span> <span class="o">*</span> <span class="mf">0.6</span>
<span class="n">super_img</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">uint8</span><span class="p">(</span><span class="n">super_img</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">matshow</span><span class="p">(</span><span class="n">super_img</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<font color="red"><b>ToSubmit 2</b></font>
<p>In your report, include the two Grad-CAM heatmaps for the image with two animals. Add a caption explaining very briefly (1 or 2 sentences) whether there’s a difference and why.</p>

<h3 id="path-integration-methods---integrated-gradients-ig">Path-integration methods - Integrated Gradients (IG)</h3>

<p>As a reminder, the problem that want to study in this tutorial is to find a way to attribute importance in the input features of the vector $\mathbf{x}_i \in \mathbb{R}^{D}$ given the result of the classification from a classifier $\mathcal{F}$.</p>

<p>Suppose that we have a funtion $\mathcal{F}: \mathbb{R}^{D} \to [0, 0, … , 1, … , 0, 0] \in \mathbb{R}^{M} $ which represent a neural network. The input to this network are data $\mathbf{X} = (\mathbf{x}_1, \mathbf{x}_2, …, \mathbf{x}_n) \in \mathbb{R}^{N\times D}$ we would like to calculate a vector $\mathbf{\alpha}_0 = (\alpha_1, \alpha_2, …, \alpha_D) \in \mathbb{R}^{D}$ which is the contribution of the input vector $\mathbf{x}_0$ to the prediction $\mathcal{F}(\mathbf{x}_i)$.</p>

<p>Path-attribution methods in contrast with the gradient methods that we have mentioned before (saliency maps and grad-cam) compare the current image  $\mathbf{x}$ to a reference image $\mathbf{x}^{\prime}$ which can be for instance a black image (or a white image or an image containing random noise). The difference in actual and baseline prediction is divided among the pixels.</p>

<h3 id="ig-approach">IG approach</h3>

<p>As a reminder, the problem that want to study in this tutorial is to find a way to attribute importance in the input features of the vector $\mathbf{x}_i \in \mathbb{R}^{D}$ given the result of the classification from a classifier $\mathcal{F}$.</p>

<p>Suppose that we have a funtion $\mathcal{F}: \mathbb{R}^{D} \to [0, 0, … , 1, … , 0, 0] \in \mathbb{R}^{M} $ which represent a neural network. The input to this network are data $\mathbf{X} = (\mathbf{x}_1, \mathbf{x}_2, …, \mathbf{x}_n) \in \mathbb{R}^{N\times D}$ we would like to calculate a vector $\mathbf{\alpha}_0 = (\alpha_1, \alpha_2, …, \alpha_D) \in \mathbb{R}^{D}$ which is the contribution of the input vector $\mathbf{x}_0$ to the prediction $\mathcal{F}(\mathbf{x}_i)$.</p>

<p>Path-attribution methods in contrast with the gradient methods that we have mentioned before (saliency maps and grad-cam) compare the current image  $\mathbf{x}$ to a reference image $\mathbf{x}^{\prime}$ which can be for instance a black image (or a white image or an image containing random noise). The difference in actual and baseline prediction is divided among the pixels.</p>

<h3 id="calculate-the-integrated-gradients-with-pytorch-recipe">Calculate the integrated gradients with PyTorch recipe.</h3>

<p>Recipe for calculating the IG in our example:</p>
<ul>
  <li><strong>Choose a baseline image</strong>. You can make use of a black/white or an white noise image.</li>
  <li><strong>Build a series of inputs</strong>, each input consist of the baseline plus an additional fraction of the input-image. The final input is the baseline plus the full image. Choose your fraction at 20.</li>
  <li>For each of these inputs, <strong>calculate the gradients of the input</strong> w.r.t. the prediction (using methods under 2 above). Take the average of all these gradients.</li>
  <li><strong>Calculate the difference of image and baseline</strong>: I-B. And calculate Integrated Gradient = (I-B)*average of gradients.</li>
  <li>If you have chosen for another baseline, e.g. for a uniform random generated baseline, then perform this procedure for multiple samples.</li>
</ul>

<h4 id="integrated-gradients-with-one-baseline">Integrated Gradients with one baseline</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="rouge-code"><pre><span class="c1"># integrated gradients
</span><span class="k">def</span> <span class="nf">integrated_gradients</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">target_label_idx</span><span class="p">,</span> <span class="n">baseline</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cuda</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="c1"># determine baseline
</span>    <span class="k">if</span> <span class="n">baseline</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">baseline</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">*</span> <span class="n">inputs</span> 
    <span class="c1"># scale inputs and compute gradients
</span>    <span class="n">scaled_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">baseline</span> <span class="o">+</span> <span class="p">(</span><span class="nf">float</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">/</span> <span class="n">steps</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">inputs</span> <span class="o">-</span> <span class="n">baseline</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="n">grads</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">calculate_outputs_and_gradients</span><span class="p">(</span><span class="n">scaled_inputs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">target_label_idx</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>

   <span class="c1"># BEGIN students TODO
</span>    <span class="n">avg_grads</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">average</span><span class="p">(</span><span class="n">grads</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  
    <span class="n">avg_grads</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">avg_grads</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">delta_X</span> <span class="o">=</span> <span class="p">(</span><span class="nf">pre_processing</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span> <span class="o">-</span> <span class="nf">pre_processing</span><span class="p">(</span><span class="n">baseline</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)).</span><span class="nf">detach</span><span class="p">().</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
    <span class="n">delta_X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">delta_X</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">integrated_grad</span> <span class="o">=</span> <span class="n">delta_X</span> <span class="o">*</span> <span class="n">avg_grads</span>
    <span class="c1"># END students TODO
</span>    <span class="k">return</span> <span class="n">integrated_grad</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="integrated-gradients-with-a-sample-of-random-baselines">Integrated Gradients with a sample of random baselines</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">random_baseline_integrated_gradients</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">target_label_idx</span><span class="p">,</span> <span class="n">steps</span><span class="p">,</span> <span class="n">num_random_trials</span><span class="p">,</span> <span class="n">cuda</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="c1"># when baseline randomly generated, take some samples and average result
</span>    <span class="c1"># BEGIN students TODO
</span>    <span class="n">all_intgrads</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">random_baseline</span> <span class="o">=</span> <span class="mf">255.0</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_random_trials</span><span class="p">):</span>
        <span class="n">integrated_grad</span> <span class="o">=</span> <span class="nf">integrated_gradients</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">target_label_idx</span><span class="p">,</span> <span class="n">baseline</span><span class="o">=</span><span class="n">random_baseline</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">,</span> <span class="n">cuda</span><span class="o">=</span><span class="n">cuda</span><span class="p">)</span>
        <span class="n">all_intgrads</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">integrated_grad</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">the trial number is: {}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
    <span class="n">avg_intgrads</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">average</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">all_intgrads</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># END students TODO
</span>    <span class="k">return</span> <span class="n">avg_intgrads</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<font color="green"><b>ToDo 4</b></font>
<p>Investigate how well integrated gradients can determine what parts of the image your models is looking at for different target categories. Also investigate whether the zero baseline or a sample of random baselines gives you clearer feature attributions.</p>

<p>Code snippets that might be useful:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="rouge-code"><pre># calculate the integrated gradients 
print('img:', img.shape, 'label_index', label_index)

# for zero baseline
int_gradients_zerobl = integrated_gradients(img, model, label_index, baseline=None, steps=50, cuda=cuda)
print('DONE')
# for random baselines, we average over number of trials
int_gradients_randombl = random_baseline_integrated_gradients(img, model, label_index, steps=50, num_random_trials=5, cuda=cuda)    
print('DONE')

# calculate saliency
gradients, _ = calculate_outputs_and_gradients([img], model, None, cuda) 
gradients = np.transpose(gradients[0], (1, 2, 0))

# combine it all in one image
plot_gradients(img, gradients, 'The Image and Its Saliency Map')
plot_gradients(img, int_gradients_zerobl, 'Image and Integrated Gradients with Zero Baseline')
plot_gradients(img, int_gradients_randombl, 'Image and Integrated Gradients with sample of Random Baselines')
</pre></td></tr></tbody></table></code></pre></div></div>

<h1 id="homework-part-1-tosubmit"><strong>Homework-part 1 <mark>TOSUBMIT</mark></strong>.</h1>

<p>Work for home:</p>
<ul>
  <li>Read the main article</li>
  <li>Why integraded gradients give better results than the other saliency methods?</li>
  <li>Think about what the role of the baseline is how it impact negatively the reults.</li>
  <li>Propose a way to improve the technique using alternative baselines.</li>
</ul>

<p><mark>TOSUBMIT</mark> At your first paragraph of your report you should analyze this.</p>]]></content><author><name>Christos Athanasiadis</name></author><summary type="html"><![CDATA[This page's main focus is to analyze a branch of explainable & interpretable AI (XAI) called posthoc XAI. We will analyze theory, taxonomy, applications, shortcomings of posthoc XAI approaches and apply them on image classification using popular CNN architectures and explain their black box nature. Part of the assessemnet for this tutorial/workshop, will be some research questions that needs be answered by you. These questions can be found all over this blogspot using the TOSUBMIT tag and will be summarized them at the end of the blogspot.]]></summary></entry><entry><title type="html">How to explain the behavior of vision transformers?</title><link href="https://kristosh.github.io//blog/2023/Explainable-ViT/" rel="alternate" type="text/html" title="How to explain the behavior of vision transformers?" /><published>2023-05-13T00:00:00+00:00</published><updated>2023-05-13T00:00:00+00:00</updated><id>https://kristosh.github.io//blog/2023/Explainable-ViT</id><content type="html" xml:base="https://kristosh.github.io//blog/2023/Explainable-ViT/"><![CDATA[<h1 id="introduction">Introduction</h1>

<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2023-06-23-Explainable-ViT/ViT_architecture.PNG" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>So what is a Vision Transformer? What is going on with the inner parameters of it? How do they even work? Can we poke at these parameters and dissect them into pieces to understand them better?</p>

<p>These are some fundamental questions that we will try to answer in this post. Firstly, we will try to remind our reader about what is exactly a vision transformer and how it works. We will develop a simple image classifier that distinguishes between 🍕🍣🥩 images. Moreover, we will try to showcase methods that aim to shed light on the inner mechanisms of the <code class="language-plaintext highlighter-rouge">ViT</code> model. These visualizations could be useful for:</p>

<ul>
  <li>
    <p>Figuring out which parts of the transformers are activated when we input a specific image. Being able to look at intermediate activation layers different heads and part of the architecture and investigate what led to specific model activation.</p>
  </li>
  <li>
    <p>Figuring out what did it learn? What type of patterns did the model learn? Usually, this is in the form of the question <em>What input image maximizes the response from this activation?</em>, and you can use variants of <em>Activation Maximization</em> for that.</p>
  </li>
  <li>
    <p>Figuring out what did it see in this image? Being able to Answer <em>What part of the image is responsible for the network prediction</em>, is sometimes called <em>feature or pixel attribution</em>.</p>
  </li>
</ul>

<p>We will make use of <code class="language-plaintext highlighter-rouge">PyTorch</code> to implement these methods and showcase the results. Moreover, we will make use of two different XAI methods. At the end of this tutorial, a simple TODO exercise will be provided to gauge the performance of different XAI methods for our built <code class="language-plaintext highlighter-rouge">ViT</code> model.</p>

<h1 id="vision-transformer-vit">Vision Transformer ViT</h1>

<p>The vanilla Transformer architecture was introduced by Vaswani et al. in 2017 [1], to tackle sequential data and particularly textual information for the machine translation task. Given the success of the Transformer in the NLP domain, Dosovitskiy et al. [2] proposed the Vision Transformer (<code class="language-plaintext highlighter-rouge">ViT</code>) architecture for visual classification tasks. The <code class="language-plaintext highlighter-rouge">ViT</code> architecture is the standard transformer architecture but with visual information as input instead. In the <code class="language-plaintext highlighter-rouge">ViT</code> context, we need to convert the <code class="language-plaintext highlighter-rouge">3D</code> grid of pixels into a sequence of token embeddings. This could be done by splitting the image into non-overlapping patches and then, each patch should be flattened into a <code class="language-plaintext highlighter-rouge">1D</code> vector and then linearly projected into a vector of token embeddings. Finally, these token embeddings are fed into the <code class="language-plaintext highlighter-rouge">ViT</code> architecture in a similar way as the vanilla transformers. The basic blocks of the <code class="language-plaintext highlighter-rouge">ViT</code> architecture can be seen in the previous image and are:</p>

<ul>
  <li><strong>inputs</strong></li>
  <li><strong>linear projection (embedding layer)</strong></li>
  <li><strong>positional encoding</strong></li>
  <li><strong>transformer encoder</strong>
    <ul>
      <li><strong>Layer normalization</strong></li>
      <li><strong>multi-head self-attention</strong></li>
      <li><strong>feed-forward neural network</strong></li>
    </ul>
  </li>
  <li><strong>classification token</strong></li>
  <li><strong>MLP head</strong></li>
  <li><strong>output predictions</strong></li>
</ul>

<p>To help the reader comprehend all the above, we will provide a simple grouping of definitions and use the following two terms:</p>

<ul>
  <li><strong>layers</strong>: are basic elements that are used to build the architecture. For example, the multi-head self-attention can be considered as a layer. It’s important to mention that a Transformer is composed usually of several Encoders. Each of these Enconders can be referred to in the literature as a layer as well.</li>
  <li><strong>blocks</strong>: a grouping of layers (for instance the whole encoder can be seen as a block of layers).</li>
</ul>

<p>The ViTs architecture is comprised of several stages:</p>

<ul>
  <li><strong>Patch + Position Embedding (inputs)</strong>: Turns the input image into a sequence of image patches and adds a position number to specify in what order the patch comes in.</li>
  <li><strong>Linear projection of flattened patches (Embedded Patches)</strong>: The image patches are flattened and then projected into embeddings. The benefit of using embeddings rather than just the image values is that embeddings are learnable representations of the image that can improve with training.</li>
  <li><strong>Norm</strong>: This is short for <code class="language-plaintext highlighter-rouge">Layer Normalization</code> or <code class="language-plaintext highlighter-rouge">LayerNorm</code>, a technique for regularizing (reducing overfitting) a neural network, you can use LayerNorm via the <code class="language-plaintext highlighter-rouge">PyTorch</code> layer <code class="language-plaintext highlighter-rouge">torch.nn.LayerNorm()</code>.</li>
  <li><strong>Multi-Head Attention</strong>: This is a Multi-Headed Self-Attention layer or <code class="language-plaintext highlighter-rouge">MSA</code> for short. You can create an MSA layer via the PyTorch layer <code class="language-plaintext highlighter-rouge">torch.nn.MultiheadAttention()</code>.</li>
  <li><strong>MLP (or Multilayer perceptron)</strong>: An MLP can often refer to any collection of feedforward layers (or in PyTorch’s case, a collection of layers with a forward() method). In the <code class="language-plaintext highlighter-rouge">ViT</code> Paper, the authors refer to the MLP as “MLP block” and it contains two <code class="language-plaintext highlighter-rouge">torch.nn.Linear()</code> layers with a <code class="language-plaintext highlighter-rouge">torch.nn.GELU()</code> non-linearity activation in between them (section 3.1) and a <code class="language-plaintext highlighter-rouge">torch.nn.Dropout()</code> layer after each.</li>
  <li><strong>Transformer Encoder</strong>: The Transformer Encoder, is a collection of the layers listed above. There are two skip connections inside the Transformer encoder (the “+” symbols) meaning the layer’s inputs are fed directly to immediate layers as well as subsequent layers. The overall <code class="language-plaintext highlighter-rouge">ViT</code> architecture is comprised of several Transformer encoders stacked on top of each other.</li>
  <li><strong>MLP Head</strong>: This is the output layer of the architecture, it converts the learned features of an input to a class output. Since we’re working on image classification, you could also call this the “classifier head”. The structure of the <code class="language-plaintext highlighter-rouge">MLP</code> Head is similar to the MLP block.</li>
</ul>

<p>Breaking the hyperparameters down:</p>

<ul>
  <li><strong>Layers</strong>: How many Transformer Encoder blocks are there? (each of these will contain an MSA block and the MLP block)</li>
  <li><strong>Hidden size  D</strong>: This is the embedding dimension throughout the architecture, this will be the size of the vector that our image gets turned into when it gets patched and embedded. Generally, the larger the embedding dimension, the more information can be captured, which leads to better results. However, a larger embedding comes at the cost of more computing. One niche issue here is that we need to distinguish the <em>layers</em> as hyperparameters that refer to the number of Transformer Encoder blocks and the <em>layer</em> as a building block of the encoder.</li>
  <li><strong>MLP size</strong>: What are the number of hidden units in the MLP layers?</li>
  <li><strong>Heads</strong>: How many heads are there in the Multi-Head Attention layers?
<!-- - **Params**: What is the total number of parameters of the model? Generally, more parameters lead to better performance but at the cost of more compute. You'll notice even ViT-Base has far more parameters than any other model we've used so far. --></li>
</ul>

<h2 id="vision-transformers-vits-tokenization">Vision Transformers (ViTs) Tokenization</h2>

<p>The standard Transformer receives as input a 1D sequence of token embeddings. To handle 3D images, we reshape the image $\mathbf{x} \in \mathbb{R}^{H \times W \times C}$ into a sequence of flattened patches with size $\mathbf{x}_P \in \mathbb{R}^{N \times CP^2}$, when $H, W$ represent the height and the width of an image while $C$ represents the number of channels, then, $N$ is the number of patches and $P$ is the patch dimensionality. The Transformer uses constant latent vector size  $D$ through all of its layers, so we flatten the patches and map to $D$ dimensions with a trainable linear projection (Equation 1 from the <code class="language-plaintext highlighter-rouge">ViT</code> paper). We refer to the output of this projection as the patch embeddings. If the input image is of size $224 \times 224 \times 3$ and the patch size is $16$ then the output should be of size $196 \times 768$, where the first dimension is the number of patches and the second dimension is the size of the patch embeddings $16\cdot 16\cdot 3 = 768$.</p>

<!-- - is is the same with the transformer enconder? -->
<h2 id="transformer-enconder">Transformer enconder</h2>

<p>After having created the patches, we should proceed with the implementation of the transformer encoder which can be seen in Figure 1. It can mainly divided into the <code class="language-plaintext highlighter-rouge">multi-head attention</code> (MSA) and the <code class="language-plaintext highlighter-rouge">MLP</code> layer. The multi-head self-attention mechanism is used to capture the dependencies between the patches. The feed-forward neural network is used to capture the non-linear interactions between the patches. The following image portrays the mechanism of the attention block.</p>

<p>Here we will need to decide whether the input to our model will be the full image or the image patches. To decide that we will take into account that a lot of pre-trained models have as input the full image. Thus, for now, we will use the full image as input to our model.</p>

<p>Moreover, note that we make use also of <code class="language-plaintext highlighter-rouge">Layer normalization</code> …</p>

<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2023-06-23-Explainable-ViT/ViT_architecture.PNG" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h2 id="layer-normalization">Layer normalization</h2>

<p>The LayerNorm (LN) Layer Normalization (<code class="language-plaintext highlighter-rouge">torch.nn.LayerNorm()</code> or <code class="language-plaintext highlighter-rouge">Norm</code> or <code class="language-plaintext highlighter-rouge">LayerNorm</code> or <code class="language-plaintext highlighter-rouge">LN</code>) normalizes an input over the last dimension. PyTorch’s <code class="language-plaintext highlighter-rouge">torch.nn.LayerNorm()</code> main parameter is normalized_shape which we can set to be equal to the dimension size we’d like to normalize over (in our case it’ll be  $D$ or $768$ for <code class="language-plaintext highlighter-rouge">ViT</code>-Base). What does it do?</p>

<p>Layer Normalization helps improve training time and model generalization (ability to adapt to unseen data). I like to think of any kind of normalization as “getting the data into a similar format” or “getting data samples into a similar distribution”. Imagine trying to walk up (or down) a set of stairs all with differing heights and lengths.</p>

<p>It’d take some adjustment on each step, right? And what you learn for each step wouldn’t necessarily help with the next one since they all differ, increasing the time it takes you to navigate the stairs. Normalization (including <code class="language-plaintext highlighter-rouge">Layer Normalization</code>) is the equivalent of making all the stairs the same height and length except the stairs are your data samples. So just like you can walk up (or down) stairs with similar heights and lengths much easier than those with unequal heights and widths, neural networks can optimize over data samples with similar distributions (similar mean and standard deviations) easier than those with varying distributions.</p>

<h1 id="pizza-sushi-steak--classifier">Pizza-sushi-steak 🍕🍣🥩 classifier</h1>

<p>Now it’s time to code all the above into a classifier. We will implement a simple <code class="language-plaintext highlighter-rouge">ViT</code> classifier for the pizza-sushi-steak dataset. The dataset contains train and test folders with $450$ images for training and 150 images for testing. We will start by providing some code for setting up our data. Firstly, we should download the dataset:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="n">image_path</span> <span class="o">=</span> <span class="nf">download_data</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="sh">"</span><span class="s">https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">destination</span><span class="o">=</span><span class="sh">"</span><span class="s">pizza_steak_sushi</span><span class="sh">"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Set up the paths for the training and testing data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="n">data_path</span> <span class="o">=</span> <span class="nc">Path</span><span class="p">(</span><span class="sh">"</span><span class="s">data/</span><span class="sh">"</span><span class="p">)</span>
<span class="n">image_path</span> <span class="o">=</span> <span class="n">data_path</span> <span class="o">/</span> <span class="sh">"</span><span class="s">pizza_steak_sushi</span><span class="sh">"</span>

<span class="c1"># Setup directory paths to train and test images
</span><span class="n">train_dir</span> <span class="o">=</span> <span class="n">image_path</span> <span class="o">/</span> <span class="sh">"</span><span class="s">train</span><span class="sh">"</span>
<span class="n">test_dir</span> <span class="o">=</span> <span class="n">image_path</span> <span class="o">/</span> <span class="sh">"</span><span class="s">test</span><span class="sh">"</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Then, we would like to perform some basic transformations to our data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="c1"># Create image size (from Table 3 in the ViT paper)
</span><span class="n">IMG_SIZE</span> <span class="o">=</span> <span class="mi">224</span>

<span class="c1"># Create transform pipeline manually
</span><span class="n">manual_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">Resize</span><span class="p">((</span><span class="n">IMG_SIZE</span><span class="p">,</span> <span class="n">IMG_SIZE</span><span class="p">)),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
<span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Then you will need to create the necessary dataloaders for train and test sets:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre></td><td class="rouge-code"><pre><span class="c1"># Use ImageFolder to create dataset(s)
</span>  <span class="n">train_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">ImageFolder</span><span class="p">(</span><span class="n">train_dir</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
  <span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">ImageFolder</span><span class="p">(</span><span class="n">test_dir</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>

  <span class="c1"># Get class names
</span>  <span class="n">class_names</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">.</span><span class="n">classes</span>

  <span class="c1"># Turn images into data loaders
</span>  <span class="n">train_dataloader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span>
      <span class="n">train_data</span><span class="p">,</span>
      <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
      <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
      <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
      <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
  <span class="p">)</span>
  
  <span class="n">test_dataloader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span>
      <span class="n">test_data</span><span class="p">,</span>
      <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
      <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="c1"># don't need to shuffle test data
</span>      <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
      <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
  <span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Check the repository (here) for the implementations for the Dataloaders. As a follow-up step, we should return a batch of images and labels with the following code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">image_batch</span><span class="p">,</span> <span class="n">label_batch</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="nf">iter</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<h1 id="create-a-vit-classifier">Create a ViT classifier</h1>

<p>Having loaded the data, now it’s time to introduce a simple <code class="language-plaintext highlighter-rouge">ViT</code> model and fit our data. We will call the <code class="language-plaintext highlighter-rouge">ViT</code> model and pass the image batch to it. The output of the model will be the logits for each class. We will then use the cross-entropy loss to calculate the loss and the Adam optimizer to update the weights of the model. The following code will help you to create a simple <code class="language-plaintext highlighter-rouge">ViT</code> classifier:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td><td class="rouge-code"><pre><span class="n">device</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="nf">set_seeds</span><span class="p">()</span>
<span class="c1"># Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi)
</span><span class="n">vit</span> <span class="o">=</span> <span class="nc">ViT</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">cls_names</span><span class="p">))</span>

<span class="c1"># Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">vit</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">3e-3</span><span class="p">,</span> <span class="c1"># Base LR from Table 3 for ViT-* ImageNet-1k
</span>    <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="c1"># default values but also mentioned in ViT paper section 4.1 (Training &amp; Fine-tuning)
</span>    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span> <span class="c1"># from the ViT paper section 4.1 (Training &amp; Fine-tuning) and Table 3 for ViT-* ImageNet-1k
</span>
<span class="c1"># Setup the loss function for multi-class classification
</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="c1"># Train the model and save the training results to a dictionary
</span><span class="n">results</span> <span class="o">=</span> <span class="nf">train_function</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">vit</span><span class="p">,</span>
    <span class="n">train_dataloader</span><span class="o">=</span><span class="n">train_dataloader</span><span class="p">,</span>
    <span class="n">test_dataloader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="building-the-vit-model">Building the ViT model</h2>

<p>Of course, we will need to develop the code for the <code class="language-plaintext highlighter-rouge">ViT</code> model as well. That is a bit more complicated. At first, we will illustrate the whole code and then, we will analyze it step by step. The code looks as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
</pre></td><td class="rouge-code"><pre><span class="c1"># 1. Create a ViT class that inherits from nn.Module
</span><span class="k">class</span> <span class="nc">ViT</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.</span><span class="sh">"""</span>
    <span class="c1"># Initialize the class with hyperparameters from Table 1 and Table 3 from original ViT paper
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>
            <span class="n">img_size</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="c1"># Training resolution from Table 3 in ViT paper
</span>            <span class="n">in_channels</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="c1"># Number of channels in input image
</span>            <span class="n">patch_size</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="c1"># Patch size
</span>            <span class="n">num_transformer_layers</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="c1"># Layers from Table 1 for ViT-paper
</span>            <span class="n">embedding_dim</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="c1"># Hidden size D from Table 1 for ViT-paper
</span>            <span class="n">mlp_size</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span> <span class="c1"># MLP size from Table 1 for ViT-paper
</span>            <span class="n">num_heads</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="c1"># Heads from Table 1 for ViT-paper
</span>            <span class="n">attn_dropout</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">mlp_dropout</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
            <span class="n">embedding_dropout</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
            <span class="n">num_classes</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span> <span class="c1"># The nubmer of classes in the dataset
</span>        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span> <span class="c1"># inherited initialization from nn.Module
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="p">(</span><span class="n">img_size</span> <span class="o">*</span> <span class="n">img_size</span><span class="p">)</span> <span class="o">//</span> <span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span> <span class="c1"># Calculate number of patches (height * width/patch^2) 
</span>        <span class="n">self</span><span class="p">.</span><span class="n">class_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">),</span> <span class="c1"># Create learnable class embedding
</span>            <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">position_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_patches</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">),</span>  <span class="c1"># Create learnable position embedding
</span>            <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">embedding_dropout</span><span class="p">)</span>  <span class="c1"># Create embedding dropout value
</span>        <span class="n">self</span><span class="p">.</span><span class="n">patch_embedding</span> <span class="o">=</span> <span class="nc">PatchEmbedding</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="c1"># Create patch embedding layer
</span>            <span class="n">patch_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
            <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">)</span>

        <span class="c1"># 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())
</span>        <span class="c1"># Note: The "*" means "all"
</span>        <span class="n">self</span><span class="p">.</span><span class="n">transformer_encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="nc">TransformerEncoderBlock</span><span class="p">(</span><span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">mlp_size</span><span class="o">=</span><span class="n">mlp_size</span><span class="p">,</span>
            <span class="n">mlp_dropout</span><span class="o">=</span><span class="n">mlp_dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_transformer_layers</span><span class="p">)])</span>

        <span class="c1"># 10. Create classifier head
</span>        <span class="n">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
                      <span class="n">out_features</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># 11. Create a forward() method
</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Get batch size
</span>        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># Create class token embedding and expand it to match the batch size (equation 1)
</span>        <span class="n">class_token</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">class_embedding</span><span class="p">.</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># "-1" means to infer the dimension (try this line on its own)
</span>        <span class="c1"># Create patch embedding (equation 1)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">patch_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Concat class embedding and patch embedding (equation 1)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">class_token</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Add position embedding to patch embedding (equation 1)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">position_embedding</span> <span class="o">+</span> <span class="n">x</span>
        <span class="c1"># Run embedding dropout (Appendix B.1)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding_dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Pass patch, position and class embedding through transformer encoder layers (equations 2 &amp; 3)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transformer_encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Put 0 index logit through classifier (equation 4)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="c1"># run on each sample in a batch at 0 index
</span>        <span class="k">return</span> <span class="n">x</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<h3 id="breaking-down-the-code-step-by-step">Breaking down the code step-by-step</h3>

<p>Firstly, as mentioned in the code block comments we follow the details of the ViT paper such as <code class="language-plaintext highlighter-rouge">batch size</code>, <code class="language-plaintext highlighter-rouge">number of patches</code>, <code class="language-plaintext highlighter-rouge">number of layers</code>, the <code class="language-plaintext highlighter-rouge">dimensionality of the embeddings</code>, <code class="language-plaintext highlighter-rouge">number of heads</code>, etc. More details can be found in the paper’s Table 1 and Table 3.</p>

<p>The first thing that our code tries to emulate is the creation of patches. Given, an image we create patches of size $16 \times 16$ ($P \times P$). Thus, if the input image has size $H \times W \times C$  and is $224 \times 224 \times 3$, the total amount of patches is $N = 196$, and can be calculated by the following formula $N = HW/P^{2}$. Then, these image patches are turned into embeddings, by using the <code class="language-plaintext highlighter-rouge">PatchEmbedding</code> functionality. The benefit of turning the raw images into embeddings is that we can learn a representation of the image that can improve with training.</p>

<p>Different values for the size of the embeddings can be found in Table 1, but throughout this tutorial, we will make use of $D = 768$. The idea is to first split the image into patches and then apply a learnable 2d convolutional layer to each patch. If we set the proper values for the kernel_size and stride parameters of a <code class="language-plaintext highlighter-rouge">torch.nn.Conv2d()</code> then we can have the desired output embedding, for instance, $D = 768$ in our case. To facilitate the dimensions of output smoothly we will need to make use of a <code class="language-plaintext highlighter-rouge">flatten()</code> function to flatten the output of the convolutional layer.</p>

<p>The next step is to stack $m$ Transformer Encoders together using the following code: <code class="language-plaintext highlighter-rouge">nn.Sequential(*[TransformerEncoderBlock(.)</code> and finally add a linear layer that will output the desired amount of classes <code class="language-plaintext highlighter-rouge">nn.Linear(in_features=embedding_dim, out_features=num_classes)</code>.</p>

<p><strong>PatchEmbedding code</strong>: After having created the patches in the main <code class="language-plaintext highlighter-rouge">ViT</code> class, the next step is to calculate the embeddings of the patches. This is done by the <code class="language-plaintext highlighter-rouge">PatchEmbedding</code> class. The code for the <code class="language-plaintext highlighter-rouge">PatchEmbedding</code> class is as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">PatchEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Turns a 2D input image into a 1D sequence learnable embedding vector.

    Args:
        in_channels (int): Number of color channels for the input images. Defaults to 3.
        patch_size (int): Size of patches to convert input image into. Defaults to 16.
        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.
    </span><span class="sh">"""</span>
    <span class="c1"># 2. Initialize the class with appropriate variables
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">768</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">patcher</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># only flatten the feature map dimensions into a single vector
</span>            
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Create assertion to check that inputs are the correct shape
</span>        <span class="n">image_resolution</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Perform the forward pass
</span>        <span class="n">x_patched</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">patcher</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x_flattened</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="n">x_patched</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_flattened</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -&gt; [batch_size, N, P^2•C]
</span></pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>TransformerEncoderBlock code</strong>: The second main part of the code is the <code class="language-plaintext highlighter-rouge">TransformerEncoderBlock</code> class. This class is responsible for the creation of the Transformer Encoder block. It is mainly composed as Figure 1 portrays in two parts: <code class="language-plaintext highlighter-rouge">MultiheadSelfAttentionBlock</code> and <code class="language-plaintext highlighter-rouge">MLPBlock</code> blocks. The code for the <code class="language-plaintext highlighter-rouge">TransformerEncoderBlock</code> class is as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">TransformerEncoderBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Creates a Transformer Encoder block.</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>
            <span class="n">embedding_dim</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> 
            <span class="n">num_heads</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> 
            <span class="n">mlp_size</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span> 
            <span class="n">mlp_dropout</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
            <span class="n">attn_dropout</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span> 
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">msa_block</span> <span class="o">=</span> <span class="nc">MultiheadSelfAttentionBlock</span><span class="p">(</span><span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">attn_dropout</span><span class="o">=</span><span class="n">attn_dropout</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">mlp_block</span> <span class="o">=</span> <span class="nc">MLPBlock</span><span class="p">(</span><span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="c1"># You can find more information for this part of the code in the repository
</span>            <span class="n">mlp_size</span><span class="o">=</span><span class="n">mlp_size</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">mlp_dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span>  <span class="n">self</span><span class="p">.</span><span class="nf">msa_block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mlp_block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="training-function-for-the-vit-model">Training function for the ViT model</h2>

<p>Having defined already the dataset, our model and the loss function, we can directly proceed with the training of our <code class="language-plaintext highlighter-rouge">ViT</code> model. The idea is to iterate through all the epochs and batches and update the parameters of the model using backpropagation as usual. Then, we should report the loss and the accuracy of the model for the training and test sets. The code should look as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="rouge-code"><pre><span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="mi">0</span>    
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
        <span class="n">X</span> <span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span> 
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="n">y_pred_class</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">train_acc</span> <span class="o">+=</span> <span class="p">(</span><span class="n">y_pred_class</span> <span class="o">==</span> <span class="n">y</span><span class="p">).</span><span class="nf">sum</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>

<span class="n">train_loss</span> <span class="o">=</span> <span class="n">train_loss</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
<span class="n">train_acc</span> <span class="o">=</span> <span class="n">train_acc</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Of course, we will need to measure also the performance in the test set as usual. The code is identical to the training process and can be found in the repository.</p>

<h2 id="measuring-the-performance-of-the-vit-model">Measuring the performance of the ViT model</h2>

<p>After having trained the model, we should report the performance of the model in the test set. The code should look as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
</pre></td><td class="rouge-code"><pre><span class="n">weights_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">models/pre_trained_vit_sushi_768_v2.pth</span><span class="sh">"</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">weights_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span> <span class="n">device</span><span class="p">)</span>
<span class="n">pretrained_vit</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">pretrained_vit</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>

<span class="k">if</span> <span class="n">transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">image_transform</span> <span class="o">=</span> <span class="n">transform</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">image_transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="p">.</span><span class="nc">Resize</span><span class="p">(</span><span class="n">image_size</span><span class="p">),</span>
        <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
            <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]),])</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">inference_mode</span><span class="p">():</span>
    <span class="n">transformed_image</span> <span class="o">=</span> <span class="nf">image_transform</span><span class="p">(</span><span class="n">img</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">target_image_pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">transformed_image</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

<span class="n">target_image_pred_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">target_image_pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">target_image_pred_label</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">target_image_pred_probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Plot the results
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Pred: </span><span class="si">{</span><span class="n">class_names</span><span class="p">[</span><span class="n">target_image_pred_label</span><span class="p">]</span><span class="si">}</span><span class="s"> | Prob: </span><span class="si">{</span><span class="n">target_image_pred_probs</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Of course, you can also measure the performance of the model using the test set and extract performance metrics such as accuracy, precision, recall, and $F_1$-score. You can make use of Tensoboard to visualize the results as well.</p>

<h2 id="loading-a-pre-trained-vit-model">Loading a pre-trained ViT model</h2>

<p>As we saw in the previous section, it is not possible to train our model with only that small amount of data. Thus, we will try to perform instead Transfer learning to load pre-trained weights on <code class="language-plaintext highlighter-rouge">ImageNet</code> using the <code class="language-plaintext highlighter-rouge">ViT_B_16_Weights</code> model that comes with the <code class="language-plaintext highlighter-rouge">torchvision</code> package. Of course, this model is trained for a different target than our desired target 🍕🍣🥩. Thus, we will need to change the layers that relate to the class and replace the output with the desired amount of output layers. We will need also to freeze all the rest layers:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="c1"># Load the pre-trained ViT model        
</span><span class="n">retrained_vit_weights</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">ViT_B_16_Weights</span><span class="p">.</span><span class="n">DEFAULT</span> <span class="c1"># requires torchvision &gt;= 0.13, "DEFAULT" means best available
</span><span class="n">pretrained_vit</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nf">vit_b_16</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">retrained_vit_weights</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">pretrained_vit</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
    <span class="n">parameter</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>

<span class="n">pretrained_vit</span><span class="p">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">)).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Then, we can perform the training process as usual and report the results. Note that you could make use of the pre-trained weights of your preference, however, you should be a bit careful about the parameters that need to be updated. For instance, some pre-trained weights do not follow the same hyper-parameters as in the case of the original <code class="language-plaintext highlighter-rouge">ViT</code> paper.</p>

<h1 id="explainable-vit">Explainable ViT</h1>

<p>Until we managed to successfully train a <code class="language-plaintext highlighter-rouge">ViT</code> using the images in our handmade dataset of 🍕🍣🥩 images using transfer learning. We measure the performance in a small test set and visualize the results. But how exactly does the model classify each specific image? Which parts of the model activated and led to a specific decision? Which layers are responsible for that decision?</p>

<p>One simple way to investigate the inner mechanisms of the <code class="language-plaintext highlighter-rouge">ViT</code> model is to visualize the attention weights which is the easiest and most popular approach to interpret a model’s decisions and to gain insights about its internals. These weights are calculated by the <code class="language-plaintext highlighter-rouge">Multi-Head Self-Attention</code> mechanism and can help us to understand which parts of the image are responsible for the decision of the model. Now, the question that pops up is: which attention maps are we going to visualize? From which layer? Which head? Remember that our model is composed of several layers and each layer (in particular we chose <code class="language-plaintext highlighter-rouge">12</code> layers).</p>

<p>Transformer model, in each layer, self-attention combines information from attended embeddings of the previous layer to compute new embeddings for each token. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed for a more thorough discussion on how the identity of tokens gets less and less represented in the embedding of that position as we go into deeper layers.</p>

<p>Hence, when looking at the $i$th self-attention layer, we can not interpret the attention weights as the attention to the input tokens, i.e., embeddings in the input layer. This makes attention weights unreliable as explanation probes to answer questions like “Which part of the input is the most important when generating the output?” (except for the very first layer where the self-attention is directly applied to the input tokens.)</p>

<p>Take home message: across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes.</p>

<p>We can start by visualizing the attention maps of one of these layers. However, this approach is not class-specific and we end up ignoring most of the attention scores. Moreover, other layers are not even considered. Somehow a more sophisticated approach to take into account all the layers is needed here.</p>

<h2 id="attention-rollout">Attention Rollout</h2>

<p>At every Transformer block, we get an attention Matrix $A_{ij}$ that defines how much attention is going to flow from image patch (token) $j$ in the previous layer to image patch (token) $i$ in the next layer. We can multiply the Matrices between every two layers, to get the total attention flow between them. Why?</p>

<p>Attention rollout and attention flow recursively compute the token attention in each layer of a given model given the embedding attention as input.
They differ in the assumptions they make about how attention weights in lower layers affect the flow of information to the higher layers and
whether to compute the token attention relative to each other or independently.</p>

<p>When we only use attention weights to approximate the flow of information in Transformers, we ignore the residual connections We can model them by adding the identity matrix $\mathbb{I}$ to the layer Attention matrices: $A_{ij}+\mathbb{I}$. We have multiple attention heads. What do we do about them? The Attention rollout paper suggests taking the average of the heads. As we will see, it can make sense using other choices: like the minimum, the maximum, or using different weights. Finally, we get a way to recursively compute the Attention Rollout matrix at layer L:</p>

\[AttentionRollout_{L}=(A_L+\mathbb{I}) AttentionRollout_{L−1}\]

<p>We also have to normalize the rows, to keep the total attention flow 1.</p>

<p>Regarding the implementation of this method, the main code for implementing the <code class="language-plaintext highlighter-rouge">Attention Rollout</code> method is as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre></td><td class="rouge-code"><pre>
<span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">attentions</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">attention</span> <span class="ow">in</span> <span class="n">attentions</span><span class="p">:</span>
        
        <span class="c1"># fusion methods 
</span>        <span class="c1">#TODO implementation
</span>
        <span class="n">flat</span> <span class="o">=</span> <span class="n">attention_heads_fused</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">attention_heads_fused</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># a list with the fused attention heads for each layer
</span>        <span class="n">_</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">flat</span><span class="p">.</span><span class="nf">topk</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">flat</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">discard_ratio</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">indices</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">flat</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">indices</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">I</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">attention_heads_fused</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># identity matrix
</span>        <span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="n">attention_heads_fused</span> <span class="o">+</span> <span class="mf">1.0</span><span class="o">*</span><span class="n">I</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span> <span class="c1"># take into account the residual connections
</span>        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">/</span> <span class="n">a</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># normalize the rows
</span>        
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">attention_heads_fused</span><span class="p">)</span> <span class="c1"># the attention rollout matrix for each layer
</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span> <span class="p">,</span> <span class="mi">1</span> <span class="p">:]</span> <span class="c1"># Look at the total attention between the class token and the image patches
</span>    <span class="c1"># In case of 224x224 image, this brings us from 196 to 14
</span>    <span class="n">width</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">mask</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">width</span><span class="p">).</span><span class="nf">numpy</span><span class="p">()</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mask</span> 
</pre></td></tr></tbody></table></code></pre></div></div>

<p>where <code class="language-plaintext highlighter-rouge">discard_ratio</code> is a hyperparameter and the variable <code class="language-plaintext highlighter-rouge">attention_heads_fused</code> represents the way that we fused the attention heads. That occurs by averaging or keeping the <code class="language-plaintext highlighter-rouge">max</code> and <code class="language-plaintext highlighter-rouge">min</code> for the attention maps.</p>

<h3 id="visual-results">Visual Results</h3>

<p align="center">
      <img src="../../../assets/img/2023-06-23-Explainable-ViT/rollout_1.png" align="left" />
      <img src="../../../assets/img/2023-06-23-Explainable-ViT/rollout_2.png" />
      <img src="../../../assets/img/2023-06-23-Explainable-ViT/rollout_3.png" align="right" />
</p>

<h3 id="cons-of-this-method">Cons of this method</h3>

<ul>
  <li>This methodology is not class-specific</li>
  <li>They end up ignoring most of the attention scores, and other layers are not even considered.</li>
  <li>
    <h2 id="gradient-attention-rollout">Gradient Attention Rollout</h2>
  </li>
</ul>

<p>The Attention that flows in the transformer passes along information belonging to different classes. Gradient rollout lets us see what locations the network paid attention too, but it tells us nothing about if it ended up using those locations for the final classification.</p>

<p>We can multiply the attention with the gradient of the target class output, and take the average among the attention heads (while masking out negative attentions) to keep only attention that contributes to the target category (or categories).</p>

<p>When fusing the attention heads in every layer, we could just weigh all the attentions (in the current implementation it’s the attentions after the softmax, but maybe it makes sense to change that) by the target class gradient, and then take the average among the attention heads</p>

<p>The main code for implementing the <code class="language-plaintext highlighter-rouge">Gradient Attention Rollout</code> method is as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre></td><td class="rouge-code"><pre><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">attention</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">attentions</span><span class="p">,</span> <span class="n">gradients</span><span class="p">):</span>                
            <span class="k">if</span> <span class="n">counter</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">continue</span>
        
            <span class="n">weights</span> <span class="o">=</span> <span class="n">grad</span>            
            <span class="n">attention_heads_fused</span> <span class="o">=</span> <span class="p">(</span><span class="n">attention</span><span class="o">*</span><span class="n">weights</span><span class="p">).</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">attention_heads_fused</span><span class="p">[</span><span class="n">attention_heads_fused</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="n">pdb</span><span class="p">.</span><span class="nf">set_trace</span><span class="p">()</span>
            <span class="c1"># Drop the lowest attentions, but
</span>            <span class="c1"># don't drop the class token
</span>            <span class="n">flat</span> <span class="o">=</span> <span class="n">attention_heads_fused</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">attention_heads_fused</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">flat</span><span class="p">.</span><span class="nf">topk</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">flat</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">discard_ratio</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
            <span class="c1">#indices = indices[indices != 0]
</span>            <span class="n">flat</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">indices</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="n">I</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">attention_heads_fused</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
            <span class="n">a1</span> <span class="o">=</span> <span class="p">(</span><span class="n">attention_heads_fused</span> <span class="o">+</span> <span class="mf">1.0</span><span class="o">*</span><span class="n">I</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
            <span class="n">a1</span> <span class="o">=</span> <span class="n">a1</span> <span class="o">/</span> <span class="n">a1</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1">#pdb.set_trace()
</span>            <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="visual-results-1">Visual Results</h3>

<p align="center">
      <img src="../../../assets/img/2023-06-23-Explainable-ViT/rollout_4.png" align="left" />
      <img src="../../../assets/img/2023-06-23-Explainable-ViT/rollout_5.png" />
      <img src="../../../assets/img/2023-06-23-Explainable-ViT/rollout_6.png" align="right" />
</p>

<h1 id="todo">TODO</h1>
<p>So far we have presented two simple methods for explainable <code class="language-plaintext highlighter-rouge">ViT</code> based on attention maps and the gradient. We have tested these methods using single images for visualization purposes from the 🍕🍣🥩 dataset. However, we haven’t yet introduced any quantified way to measure the performance of these methodologies. As a simple <code class="language-plaintext highlighter-rouge">TODO</code> you will need to come up with ways to measure the performance of these two methodologies. You will need to find a ground truth and compare both methodologies.</p>

<p>Test also the gradCAM approach for <code class="language-plaintext highlighter-rouge">ViT</code> models and compare the results with the previous methods quantitatively and quantitatively.</p>

<h1 id="conclusions">Conclusions</h1>

<p>In this tutorial, we have analyzed the <code class="language-plaintext highlighter-rouge">ViT</code> model and how it works. We have developed a simple <code class="language-plaintext highlighter-rouge">ViT</code> classifier for the 🍕🍣🥩 dataset and trained the model. We have also analyzed two approaches for explaining the behavior of the <code class="language-plaintext highlighter-rouge">ViT</code> model. The first approach called <mark>Attention Rollout</mark> is based on the <code class="language-plaintext highlighter-rouge">Attention Maps</code> and a way to summarize the content of the attention maps to understand the behavior of the model. The second approach is called <mark>Gradient Attention Rollout</mark> and is based on the <code class="language-plaintext highlighter-rouge">Gradient-based</code> methods and a way to visualize the gradient influence over the attention maps which helps as well to understand the behavior of the model. We conclude with a simple TODO exercise that will help you understand the behavior of the <code class="language-plaintext highlighter-rouge">ViT</code> model and the interpretability methods.</p>

<h1 id="references">References</h1>
<p><a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" style="font-size: smaller">[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, {. Kaiser, and I. Polosukhin. Advances in Neural Information Processing Systems, page 5998–6008. (2017).</a></p>

<p><a href="https://openreview.net/pdf?id=YicbFdNTTy" style="font-size: smaller">[2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, N, Houlsby, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, International Conference on Learning Representations (2021).</a></p>

<p><a href="https://arxiv.org/abs/2005.00928" style="font-size: smaller">[3] S. Abnar, W. Zuidema, Quantifying Attention Flow in Transformers, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020.</a></p>

<p><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.pdf" style="font-size: smaller">[4] H. Chefer, S. Gur, L. Wolf, Transformer Interpretability Beyond Attention Visualization, CVPR, 2021.</a></p>]]></content><author><name>Christos Athanasiadis</name></author><summary type="html"><![CDATA[This page's goal is to present techniques that can shed light on how Vision Transformers' models (ViTs) operate. We will first have a refresher on the ViTs and how they work. We will develop a simple ViT classifier trained on the 🍕🍣🥩 dataset and use a pre-trained model to efficiently classify the images. The next step is to introduce various methods to visualize the way that the classifier takes specific decisions. These approaches range from visualizing the attention maps to visualizing the query/key and value, but also using the backpropagated gradient similar to gradCAM algorithm. We will make use of PyTorch implementation to demonstrate some of these techniques. At the end of the blog post, there is a simple exercise that you will need to solve to portray some understanding of the way that ViTs and the interpretability methods operate.]]></summary></entry></feed>