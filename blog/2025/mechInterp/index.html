<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Mechanistic interpretability (to be filled soon) | Christos  Athanasiadis</title>
    <meta name="author" content="Christos  Athanasiadis">
    <meta name="description" content="A post on mechanistic interpretability and tranformerLens. This post is giving a basic introduction to mechanistic interpretability and ways to understand the inner mechanisms of transformer-style models like `GPT-2`. We will try to locate responsible attention maps for specific biases in several prompts and validate the causal connection by removing parts of the network. We will make use of different datasets to validate the behavior and visualize the results. Finally we will talk about circuits and attention heads.">


    <link rel="stylesheet" href="/assets/css/codeblock.scss">
    <link rel="stylesheet" href="/assets/css/codeblock.scss">

    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.3/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/.css" media="" id="highlight_theme_light">



    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%98%8E&lt;/text&gt;&lt;/svg&gt;">
    
    

    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://kristosh.github.io//blog/2025/mechInterp/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

    <link rel="stylesheet" href="/assets/css/codeblock.scss">
  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Christos </span>Athanasiadis</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Mechanistic interpretability (to be filled soon)</h1>
    <p class="post-meta">March 15, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
        ·  
        <a href="/blog/tag/xai">
          <i class="fas fa-hashtag fa-sm"></i> xAI,</a>  
          <a href="/blog/tag/interpretability">
          <i class="fas fa-hashtag fa-sm"></i> Interpretability</a>  
          
        ·  
        <a href="/blog/category/sample-posts">
          <i class="fas fa-tag fa-sm"></i> sample-posts</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <p>post on mechanistic interpretability and tranformerLens. This post is giving a basic introduction to mechanistic interpretability and ways to understand the inner mechanisms of transformer-style models like <code class="language-plaintext highlighter-rouge">GPT-2</code>. We will try to locate responsible attention maps for specific biases in several prompts and validate the causal connection by removing parts of the network. We will make use of different datasets to validate the behavior and visualize the results. Finally we will talk about circuits and attention heads.</p>

<h4 id="content-of-the-post">Content of the post</h4>

<ul>
  <li>Introduction</li>
  <li>Loading models</li>
  <li>tranformerLens</li>
  <li>Circuits</li>
</ul>

<h4 id="check-list">Check List</h4>

<ul class="task-list">
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled checked>Brush Teeth</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Put on socks
    <ul class="task-list">
      <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled checked>Put on left sock</li>
      <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Put on right sock</li>
    </ul>
  </li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled checked>Go to school</li>
</ul>

<p>Firstly, we would load several packages and libraries that will be really useful for us during the whole tutorial.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre></td>
<td class="rouge-code"><pre><span class="kn">import</span> <span class="n">circuitsvis</span> <span class="k">as</span> <span class="n">cv</span>
<span class="c1"># Testing that the library works
</span><span class="n">cv</span><span class="p">.</span><span class="n">examples</span><span class="p">.</span><span class="nf">hello</span><span class="p">(</span><span class="sh">"</span><span class="s">Neel</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Import stuff
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">einops</span>
<span class="kn">from</span> <span class="n">fancy_einsum</span> <span class="kn">import</span> <span class="n">einsum</span>
<span class="kn">import</span> <span class="n">tqdm.auto</span> <span class="k">as</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="kn">from</span> <span class="n">jaxtyping</span> <span class="kn">import</span> <span class="n">Float</span>
<span class="kn">from</span> <span class="n">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="c1"># import transformer_lens
</span><span class="kn">import</span> <span class="n">transformer_lens.utils</span> <span class="k">as</span> <span class="n">utils</span>
<span class="kn">from</span> <span class="n">transformer_lens.hook_points</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">HookPoint</span><span class="p">,</span>
<span class="p">)</span>  <span class="c1"># Hooking utilities
</span><span class="kn">from</span> <span class="n">transformer_lens</span> <span class="kn">import</span> <span class="n">HookedTransformer</span><span class="p">,</span> <span class="n">FactoredMatrix</span>

<span class="n">torch</span><span class="p">.</span><span class="nf">set_grad_enabled</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<p>We can make use of the <code class="language-plaintext highlighter-rouge">HookedTransformer</code> functionality to load our model. In this tutorial we will nake use of <code class="language-plaintext highlighter-rouge">GPT-2</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td>
<td class="rouge-code"><pre><span class="n">device</span> <span class="o">=</span> <span class="n">utils</span><span class="p">.</span><span class="nf">get_device</span><span class="p">()</span>
<span class="c1"># NBVAL_IGNORE_OUTPUT
</span><span class="n">model</span> <span class="o">=</span> <span class="n">HookedTransformer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">gpt2-small</span><span class="sh">"</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
</pre></td>
<td class="rouge-code"><pre><span class="n">model_description_text</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">## Loading Models

HookedTransformer comes loaded with &gt;40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. See my explainer for documentation of all supported models, and this table for hyper-parameters and the name used to load them. Each model is loaded into the consistent HookedTransformer architecture, designed to be clean, consistent and interpretability-friendly. 

For this demo notebook we</span><span class="sh">'</span><span class="s">ll look at GPT-2 Small, an 80M parameter model. To try the model the model out, let</span><span class="sh">'</span><span class="s">s find the loss on this paragraph!</span><span class="sh">"""</span>
<span class="n">loss</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">model_description_text</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="sh">"</span><span class="s">loss</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Model loss:</span><span class="sh">"</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

<span class="n">gpt2_text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Whats is the co-capital of Greece according to the country</span><span class="sh">'</span><span class="s">s public opinion?</span><span class="sh">"</span>
<span class="n">gpt2_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">to_tokens</span><span class="p">(</span><span class="n">gpt2_text</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">gpt2_tokens</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">gpt2_logits</span><span class="p">,</span> <span class="n">gpt2_cache</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">run_with_cache</span><span class="p">(</span><span class="n">gpt2_tokens</span><span class="p">,</span> <span class="n">remove_batch_dim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">type</span><span class="p">(</span><span class="n">gpt2_cache</span><span class="p">))</span>
<span class="n">attention_pattern</span> <span class="o">=</span> <span class="n">gpt2_cache</span><span class="p">[</span><span class="sh">"</span><span class="s">pattern</span><span class="sh">"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">"</span><span class="s">attn</span><span class="sh">"</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="n">attention_pattern</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">gpt2_str_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">to_str_tokens</span><span class="p">(</span><span class="n">gpt2_text</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Layer 0 Head Attention Patterns:</span><span class="sh">"</span><span class="p">)</span>

<span class="n">cv</span><span class="p">.</span><span class="n">attention</span><span class="p">.</span><span class="nf">attention_patterns</span><span class="p">(</span><span class="n">tokens</span><span class="o">=</span><span class="n">gpt2_str_tokens</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="n">attention_pattern</span><span class="p">)</span>

<span class="n">attn_hook_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">blocks.0.attn.hook_pattern</span><span class="sh">"</span>
<span class="n">attn_layer</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">_</span><span class="p">,</span> <span class="n">gpt2_attn_cache</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">run_with_cache</span><span class="p">(</span><span class="n">gpt2_tokens</span><span class="p">,</span> <span class="n">remove_batch_dim</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">stop_at_layer</span><span class="o">=</span><span class="n">attn_layer</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">names_filter</span><span class="o">=</span><span class="p">[</span><span class="n">attn_hook_name</span><span class="p">])</span>
<span class="n">gpt2_attn</span> <span class="o">=</span> <span class="n">gpt2_attn_cache</span><span class="p">[</span><span class="n">attn_hook_name</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">torch</span><span class="p">.</span><span class="nf">equal</span><span class="p">(</span><span class="n">gpt2_attn</span><span class="p">,</span> <span class="n">attention_pattern</span><span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/Explainable-ViT/">How to explain the behavior of vision transformers?</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/gradient_based_feature_attribution/">Gradient-based feature attribution</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/LIME/">Why should I trust you? Local Interpretable Model-agnostic Explanations</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Christos  Athanasiadis. 
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.3/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
