<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>How to explain the behavior of vision transformers? | Christos  Athanasiadis</title>
    <meta name="author" content="Christos  Athanasiadis">
    <meta name="description" content="This page's goal is to present techniques that can shed light on how Vision Transformers' models (ViTs) operate. We will first have a refresher on the ViTs and how they work. Then, we will introduce various methods to visualize the way that they function. These methods range from visualizing the attention maps to visualizing the queries keys and values, but also using the gradient similar to gradCAM approaches. We will make use of PyTorch implementation to demonstrate these methods. At the end of the blog post, there is a simple exercise that you will need to solve to portray some understanding of the way that ViT and the interpretability methods operate.">


    <link rel="stylesheet" href="/assets/css/codeblock.scss">
    <link rel="stylesheet" href="/assets/css/codeblock.scss">

    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.3/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/.css" media="" id="highlight_theme_light">



    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%98%8E&lt;/text&gt;&lt;/svg&gt;">
    
    

    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://kristosh.github.io//blog/2023/Explainable-ViT/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

    <link rel="stylesheet" href="/assets/css/codeblock.scss">

    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "How to explain the behavior of vision transformers?",
      "description": "This page's goal is to present techniques that can shed light on how Vision Transformers' models (ViTs) operate. We will first have a refresher on the ViTs and how they work. Then, we will introduce various methods to visualize the way that they function. These methods range from visualizing the attention maps to visualizing the queries keys and values, but also using the gradient similar to gradCAM approaches. We will make use of PyTorch implementation to demonstrate these methods. At the end of the blog post, there is a simple exercise that you will need to solve to portray some understanding of the way that ViT and the interpretability methods operate.",
      "published": "May 13, 2023",
      "authors": [
        {
          "author": "Christos Athanasiadis",
          "authorURL": "https://www.linkedin.com/in/christos-athanasiadis-a3b51035/",
          "affiliations": [
            {
              "name": "UvA, Interpretability and Explainability in AI",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">ChristosÂ </span>Athanasiadis</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>How to explain the behavior of vision transformers?</h1>
        <p>This page's goal is to present techniques that can shed light on how Vision Transformers' models (ViTs) operate. We will first have a refresher on the ViTs and how they work. Then, we will introduce various methods to visualize the way that they function. These methods range from visualizing the attention maps to visualizing the queries keys and values, but also using the gradient similar to gradCAM approaches. We will make use of PyTorch implementation to demonstrate these methods. At the end of the blog post, there is a simple exercise that you will need to solve to portray some understanding of the way that ViT and the interpretability methods operate.</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#vision-transformers-vits-introduction">Vision Transformers (ViTs) Introduction</a></div>
            <div><a href="#methodology">Methodology</a></div>
            <div><a href="#explainable-vits">Explainable VIts</a></div>
            <div><a href="#tosubmit">TOSUBMIT</a></div>
            <div><a href="#conclusions">Conclusions</a></div>
            <div><a href="#references">References</a></div>
            
          </nav>
        </d-contents>

        <h1 id="introduction">Introduction</h1>

<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2023-06-23-Explainable-ViT/ViT_architecture.PNG" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

<p>What is a Visual Transformer? What is going on with the inner mechanism of it? How do they even work? Can we poke at them and dissect them into pieces to understand them better? These are some questions that we will try to answer in this post. Firstly, we will try to remind our reader about what is exactly a visual transformer and how it works. Secondly, we will try to revise methods that aim to shed light on their inner mechanisms. Our aim goal is to investigate all the current standard practices for visualizing the mechanisms that cause the ViT classifier to predict a specific class each time. These visualizations could be useful for:</p>

<ul>
  <li>
    <p><strong>the developer</strong>: Whatâs going on inside when we run the Transformer on this image? Being able to look at intermediate activation layers. In computer vision - these are usually images! These are kind of interpretable since you can display the different channel activations as 2D images.</p>
  </li>
  <li>
    <p><strong>the developer</strong>: What did it learn? Being able to investigate what kind of patterns (if any) did the model learn. Usually, this is in the form of the question âWhat input image maximizes the response from this activation?â, and you can use variants of âActivation Maximizationâ for that.</p>
  </li>
  <li>
    <p><strong>both the developer and the user</strong>: What did it see in this image? Being able to Answer <em>What part of the image is responsible for the network prediction</em>, is sometimes called <em>Pixel Attribution</em>.</p>
  </li>
</ul>

<p>This tutorial was based on the code from the following tutorial: <a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformer.html" rel="external nofollow noopener" target="_blank">https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformer.html</a>.</p>

<p>What we will cover in this tutorial are the following things:</p>

<ul>
  <li>Firstly, we will create a simple ViT classifier trained in three classes. We will check how well it works and we will perform transfer-learning to improve its performance.</li>
  <li>Then, we will check several methods to shed light on how the ViT classifier works. More specifically, we will check two methods called: *Rollout methods** and <strong>Gradient-based methods</strong>
</li>
</ul>

<p>Finally, a simple TODO exercise will be provided to gauge the performance of different XAI methods for our built ViT model.</p>

<h1 id="vision-transformers">Vision Transformers</h1>

<p>The vanilla Transformer architecture was introduced by Vaswani et al. in 2017 [1], to handle sequential data and most specifically for natural language processing (NLP) for the machine translation task. Given the success of the Transformer architecture in NLP, Dosovitskiy et al. [2] proposed the Vision Transformer (ViT) architecture for image classification tasks. The ViT architecture is a pure transformer architecture that can handle images as input. Thus, unlike the vanilla Transformer, where the input was a sequence of text tokens, an image is a 3D grid of pixels. For that reason, similarly to the vanilla transformers, we will need to convert the 3D grid of pixels into a sequence of tokens. This could be done by splitting the image into non-overlapping patches. Each patch is then flattened into a 1D vector and then linearly projected into a vector of the same dimension as the token embeddings. Finally, these token embeddings are fed into the ViTs architecture in a similar way as the vanilla transformers. The basic blocks of the ViTs architecture can be seen in the previous image and are:</p>

<ul>
  <li><strong>inputs</strong></li>
  <li><strong>linear projection (embedding layer)</strong></li>
  <li><strong>positional encoding</strong></li>
  <li>
<strong>transformer encoder</strong>
    <ul>
      <li><strong>Layer normalization</strong></li>
      <li><strong>multi-head self-attention</strong></li>
      <li><strong>feed-forward neural network</strong></li>
    </ul>
  </li>
  <li><strong>classification token</strong></li>
  <li><strong>MLP head</strong></li>
  <li><strong>output predictions</strong></li>
</ul>

<p>To help the reader comprehend all the above, we will provide a simple grouping of definitions and use the following two terms:</p>

<ul>
  <li>
<strong>layers</strong>: are basic elements that are used to build the architecture. For example, the multi-head self-attention can be considered as a layer. Itâs important to mention that a Transformer is composed usually of several Encoders. Each of these Enconders can be referred to in the literature as a layer as well.</li>
  <li>
<strong>blocks</strong>: a grouping of layers (for instance the whole encoder can be seen as a block of layers), can be seen in Figure 1.</li>
</ul>

<p>The ViT architecture is comprised of several stages:</p>

<ul>
  <li>
<strong>Patch + Position Embedding (inputs)</strong>: Turns the input image into a sequence of image patches and adds a position number to specify in what order the patch comes in.</li>
  <li>
<strong>Linear projection of flattened patches (Embedded Patches)</strong>: The image patches get turned into an embedding, the benefit of using an embedding rather than just the image values is that an embedding is a learnable representation (typically in the form of a vector) of the image that can improve with training.</li>
  <li>
<strong>Norm</strong>: This is short for <em>Layer Normalization</em> or <em>LayerNorm</em>, a technique for regularizing (reducing overfitting) a neural network, you can use LayerNorm via the PyTorch layer torch.nn.LayerNorm().</li>
  <li>
<strong>Multi-Head Attention</strong>: This is a Multi-Headed Self-Attention layer or âMSAâ for short. You can create an MSA layer via the PyTorch layer  <mark style="background-color:LavenderBlush;">torch.nn.MultiheadAttention()</mark>.</li>
  <li>
<strong>MLP (or Multilayer perceptron)</strong>: An MLP can often refer to any collection of feedforward layers (or in PyTorchâs case, a collection of layers with a forward() method). In the ViT Paper, the authors refer to the MLP as âMLP blockâ and it contains two <mark style="background-color:LavenderBlush;">torch.nn.Linear()</mark> layers with a  <mark style="background-color:LavenderBlush;">torch.nn.GELU()</mark> non-linearity activation in between them (section 3.1) and a torch.nn.Dropout() layer after each.</li>
  <li>
<strong>Transformer Encoder</strong>: The Transformer Encoder, is a collection of the layers listed above. There are two skip connections inside the Transformer encoder (the â+â symbols) meaning the layerâs inputs are fed directly to immediate layers as well as subsequent layers. The overall ViT architecture is comprised of a number of Transformer encoders stacked on top of eachother.</li>
  <li>
<strong>MLP Head</strong>: This is the output layer of the architecture, it converts the learned features of an input to a class output. Since weâre working on image classification, you could also call this the âclassifier headâ. The structure of the MLP Head is similar to the MLP block.</li>
</ul>

<p>Breaking the hyperparameters down:</p>

<ul>
  <li>
<strong>Layers</strong>: How many Transformer Encoder blocks are there? (each of these will contain an MSA block and the MLP block)</li>
  <li>
<strong>Hidden size  D</strong>: This is the embedding dimension throughout the architecture, this will be the size of the vector that our image gets turned into when it gets patched and embedded. Generally, the larger the embedding dimension, the more information can be captured, which leads to better results. However, a larger embedding comes at the cost of more compute. One niche issue here is that we need to distinguish the <em>layers</em> as hyperparameters that refer to the number of Transformer Encoder blocks and the <em>layer</em> as a building block of the encoder.</li>
  <li>
<strong>MLP size</strong>: What are the number of hidden units in the MLP layers?</li>
  <li>
<strong>Heads</strong>: How many heads are there in the Multi-Head Attention layers?</li>
  <li>
<strong>Params</strong>: What is the total number of parameters of the model? Generally, more parameters lead to better performance but at the cost of more compute. Youâll notice even ViT-Base has far more parameters than any other model weâve used so far.</li>
</ul>

<h1 id="vision-transformers-vits-tokenization">Vision Transformers (ViTs) Tokenization</h1>

<p>The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image $\mathbf{x} \in \mathbb{R}^{H \times W \times C}$ into a sequence of flattened patches with size $\mathbf{x}_P \in \mathbb{R}^{N \times CP^2}$, when $H, W$ represent the height and the width of an image while $C$ represents the number of channels, then, $N$ is the number of patches and $P$ is the patch dimensionality. The Transformer uses constant latent vector size  $D$ through all of its layers, so we flatten the patches and map to $D$ dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings. If the input image is of size $224 \times 224 \times 3$ and the patch size is $16$ then the output should be of size $196 \times 768$, where the first dimension is the number of patches and the second dimension is the size of the patch embeddings $16\cdot 16\cdot 3 = 768$.</p>

<p>Having created the patches, it is time to create an implementation for the attention block which is the core unit of the ViTs architecture. The attention block is the same as the one in the vanilla transformers. It consists of a multi-head self-attention mechanism followed by a feed-forward neural network. The multi-head self-attention mechanism is used to capture the dependencies between the patches. The feed-forward neural network is used to capture the non-linear interactions between the patches. The following image portrays the mechanism of the attention block.</p>

<p>Here we will need to decide whether the input to our model will be the full image or the image patches. To decide that we will take into account that a lot of pre-trained models have as input the full image. Thus, for now, we will use the full image as input to our model.</p>

<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2023-06-23-Explainable-ViT/ViT_architecture.PNG" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

<h1 id="load-the-pizza-sushi-steak-dataset-">Load the pizza-sushi-steak dataset ðð£ð¥©</h1>

<p>In this section, we will implement a simple ViT classifier for the pizza-sushi-steak dataset. The dataset contains train and test folders with 450 images for training and 150 images for testing. We will start by providing some code for setting up our data. Firstly, we should download the dataset:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_path</span> <span class="o">=</span> <span class="nf">download_data</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="sh">"</span><span class="s">https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">destination</span><span class="o">=</span><span class="sh">"</span><span class="s">pizza_steak_sushi</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p>Set up the paths for the training and testing data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_path</span> <span class="o">=</span> <span class="nc">Path</span><span class="p">(</span><span class="sh">"</span><span class="s">data/</span><span class="sh">"</span><span class="p">)</span>
<span class="n">image_path</span> <span class="o">=</span> <span class="n">data_path</span> <span class="o">/</span> <span class="sh">"</span><span class="s">pizza_steak_sushi</span><span class="sh">"</span>

<span class="c1"># Setup directory paths to train and test images
</span><span class="n">train_dir</span> <span class="o">=</span> <span class="n">image_path</span> <span class="o">/</span> <span class="sh">"</span><span class="s">train</span><span class="sh">"</span>
<span class="n">test_dir</span> <span class="o">=</span> <span class="n">image_path</span> <span class="o">/</span> <span class="sh">"</span><span class="s">test</span><span class="sh">"</span>
</code></pre></div></div>

<p>Then, we would like to perform some basic transformations to our data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create image size (from Table 3 in the ViT paper)
</span><span class="n">IMG_SIZE</span> <span class="o">=</span> <span class="mi">224</span>

<span class="c1"># Create transform pipeline manually
</span><span class="n">manual_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">Resize</span><span class="p">((</span><span class="n">IMG_SIZE</span><span class="p">,</span> <span class="n">IMG_SIZE</span><span class="p">)),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
<span class="p">])</span>
</code></pre></div></div>

<p>Then you will need to create the dataloaders for train and test sets:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Use ImageFolder to create dataset(s)
</span>  <span class="n">train_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">ImageFolder</span><span class="p">(</span><span class="n">train_dir</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
  <span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">ImageFolder</span><span class="p">(</span><span class="n">test_dir</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>

  <span class="c1"># Get class names
</span>  <span class="n">class_names</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">.</span><span class="n">classes</span>

  <span class="c1"># Turn images into data loaders
</span>  <span class="n">train_dataloader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span>
      <span class="n">train_data</span><span class="p">,</span>
      <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
      <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
      <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
      <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
  <span class="p">)</span>
  
  <span class="n">test_dataloader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span>
      <span class="n">test_data</span><span class="p">,</span>
      <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
      <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="c1"># don't need to shuffle test data
</span>      <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
      <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
  <span class="p">)</span>
</code></pre></div></div>

<p>And return a batch of images and labels with the following code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_batch</span><span class="p">,</span> <span class="n">label_batch</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="nf">iter</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">))</span>
</code></pre></div></div>
<h1 id="create-a-vit-classifier">Create a ViT classifier</h1>

<p>Having loaded the data, now itâs time to create a simple ViT classifier and fit our data. We will call the ViT model and pass the image batch to it. The output of the model will be the logits for each class. We will then use the cross-entropy loss to calculate the loss and the Adam optimizer to update the weights of the model. The following code will help you to create a simple ViT classifier:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">device</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="nf">set_seeds</span><span class="p">()</span>
<span class="c1"># Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi)
</span><span class="n">vit</span> <span class="o">=</span> <span class="nc">ViT</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">cls_names</span><span class="p">))</span>

<span class="c1"># Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">vit</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">3e-3</span><span class="p">,</span> <span class="c1"># Base LR from Table 3 for ViT-* ImageNet-1k
</span>    <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="c1"># default values but also mentioned in ViT paper section 4.1 (Training &amp; Fine-tuning)
</span>    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span> <span class="c1"># from the ViT paper section 4.1 (Training &amp; Fine-tuning) and Table 3 for ViT-* ImageNet-1k
</span>
<span class="c1"># Setup the loss function for multi-class classification
</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="c1"># Train the model and save the training results to a dictionary
</span><span class="n">results</span> <span class="o">=</span> <span class="nf">train_function</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">vit</span><span class="p">,</span>
    <span class="n">train_dataloader</span><span class="o">=</span><span class="n">train_dataloader</span><span class="p">,</span>
    <span class="n">test_dataloader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="building-the-vit-model">Building the ViT model</h2>

<p>We have already provided the basic code on how to call the ViT model but we havenât provided the code for creating the ViT model itself. Firstly, we will present the full code for the ViT model and then we will explain it step by step. The code for the ViT model is the following:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 1. Create a ViT class that inherits from nn.Module
</span><span class="k">class</span> <span class="nc">ViT</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.</span><span class="sh">"""</span>
    <span class="c1"># Initialize the class with hyperparameters from Table 1 and Table 3 from original ViT paper
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>
            <span class="n">img_size</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="c1"># Training resolution from Table 3 in ViT paper
</span>            <span class="n">in_channels</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="c1"># Number of channels in input image
</span>            <span class="n">patch_size</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="c1"># Patch size
</span>            <span class="n">num_transformer_layers</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="c1"># Layers from Table 1 for ViT-paper
</span>            <span class="n">embedding_dim</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="c1"># Hidden size D from Table 1 for ViT-paper
</span>            <span class="n">mlp_size</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span> <span class="c1"># MLP size from Table 1 for ViT-paper
</span>            <span class="n">num_heads</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="c1"># Heads from Table 1 for ViT-paper
</span>            <span class="n">attn_dropout</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">mlp_dropout</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
            <span class="n">embedding_dropout</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
            <span class="n">num_classes</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span> <span class="c1"># The nubmer of classes in the dataset
</span>        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span> <span class="c1"># inherited initialization from nn.Module
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="p">(</span><span class="n">img_size</span> <span class="o">*</span> <span class="n">img_size</span><span class="p">)</span> <span class="o">//</span> <span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span> <span class="c1"># Calculate number of patches (height * width/patch^2) 
</span>        <span class="n">self</span><span class="p">.</span><span class="n">class_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">),</span> <span class="c1"># Create learnable class embedding
</span>            <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">position_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_patches</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">),</span>  <span class="c1"># Create learnable position embedding
</span>            <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">embedding_dropout</span><span class="p">)</span>  <span class="c1"># Create embedding dropout value
</span>        <span class="n">self</span><span class="p">.</span><span class="n">patch_embedding</span> <span class="o">=</span> <span class="nc">PatchEmbedding</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="c1"># Create patch embedding layer
</span>            <span class="n">patch_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
            <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">)</span>

        <span class="c1"># 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())
</span>        <span class="c1"># Note: The "*" means "all"
</span>        <span class="n">self</span><span class="p">.</span><span class="n">transformer_encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="nc">TransformerEncoderBlock</span><span class="p">(</span><span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">mlp_size</span><span class="o">=</span><span class="n">mlp_size</span><span class="p">,</span>
            <span class="n">mlp_dropout</span><span class="o">=</span><span class="n">mlp_dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_transformer_layers</span><span class="p">)])</span>

        <span class="c1"># 10. Create classifier head
</span>        <span class="n">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
                      <span class="n">out_features</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># 11. Create a forward() method
</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Get batch size
</span>        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># Create class token embedding and expand it to match the batch size (equation 1)
</span>        <span class="n">class_token</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">class_embedding</span><span class="p">.</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># "-1" means to infer the dimension (try this line on its own)
</span>        <span class="c1"># Create patch embedding (equation 1)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">patch_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Concat class embedding and patch embedding (equation 1)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">class_token</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Add position embedding to patch embedding (equation 1)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">position_embedding</span> <span class="o">+</span> <span class="n">x</span>
        <span class="c1"># Run embedding dropout (Appendix B.1)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding_dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Pass patch, position and class embedding through transformer encoder layers (equations 2 &amp; 3)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transformer_encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Put 0 index logit through classifier (equation 4)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="c1"># run on each sample in a batch at 0 index
</span>        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>
<h3 id="breaking-down-the-code-step-by-step">Breaking down the code step-by-step</h3>

<p>Firstly, as mentioned in the code block comments we follow the details of the ViT paper such as batch size, number of patches, number of layers, the dimensionality of the embeddings, number of heads, etc. More details can be found in the paperâs Table 1 and Table 3.</p>

<p>The first thing that our code tries to emulate is the creation of patches. Given, an image we create patches of size $16 \times 16$ ($P \times P$). Thus, if the input image has size $H \times W \times C$  and is $224 \times 224 \times 3$, the total amount of patches is $N = 196$, and can be calculated by the following formula $N = HW/P^{2}$. Then, these image patches are turned into embeddings, by using the <mark style="background-color:LavenderBlush;">PatchEmbedding</mark> functionality. The benefit of turning the raw images into embeddings is that we can learn a representation of the image that can improve with training.</p>

<p>Different values for the size of the embeddings can be found in Table 1, but throughout this tutorial, we will make use of $D = 768$. The idea is to first split the image into patches and then apply a learnable 2d convolutional layer to each patch. If we set the proper values for the kernel_size and stride parameters of a torch.nn.Conv2d() then we can have the desired output embedding for instance D = 768 in our case. To facilitate the dimensions of output smoothly we will need to make use of a flatten() function to flatten the output of the convolutional layer.</p>

<p>So far we have created the patch embeddings, but we will need to write code for the token embeddings abd also for the position embeddings. The token embeddings are created by using a learnable class embedding similar to BERT.</p>

<p>Reading the second paragraph of section 3.1 from the ViT paper, we see the following description:</p>

<h2 id="training-function-for-the-vit-model">Training function for the ViT model</h2>

<h2 id="measuring-the-performance-of-the-vit-model">Measuring the performance of the ViT model</h2>

<h2 id="loading-a-pre-trained-vit-model">Loading a pre-trained ViT model</h2>

<h1 id="explainable-vision-transformers">Explainable Vision Transformers</h1>

<h1 id="todo">TODO</h1>

<h1 id="conclusions">Conclusions</h1>

<p>In this tutorial, we have analyzed LIME a posthoc XAI technique. An explanation of how this technique works but also step-by-step the code to implement it. We have also seen how we can use LIME to explain image classifiers but also how to identify the bias in a classifier.</p>

<h1 id="references">References</h1>
<p><a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" style="font-size: smaller" rel="external nofollow noopener" target="_blank">[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, {. Kaiser, and I. Polosukhin. Advances in Neural Information Processing Systems, page 5998â6008. (2017).</a></p>

<p><a href="https://openreview.net/pdf?id=YicbFdNTTy" style="font-size: smaller" rel="external nofollow noopener" target="_blank">[2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, N, Houlsby, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, International Conference on Learning Representations (2021).</a></p>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/"></d-bibliography>
</div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        Â© Copyright 2024 Christos  Athanasiadis. 
      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
