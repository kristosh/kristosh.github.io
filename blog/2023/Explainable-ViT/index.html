<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>How to explain the behavior of vision transformers? | Christos  Athanasiadis</title>
    <meta name="author" content="Christos  Athanasiadis">
    <meta name="description" content="This page's goal is to present techniques that can shed light on how Vision Transformers' models (&lt;mark&gt;ViTs&lt;/mark&gt;) operate. We will first have a refresher on the ViTs and how they work. We will introduce a simple example of a ViT classifier trained on a pizza-sushi-steak dataset and use a pre-trained model to efficiently classify the images. Then, we will introduce various methods to visualize the way that they function. These approaches range from visualizing the attention maps to visualizing the query/key and value, but also using the backpropagated gradient similar to &lt;mark&gt;gradCAM&lt;/mark&gt; algorithm. We will make use of &lt;mark&gt;PyTorch&lt;/mark&gt; implementation to demonstrate some of these techniques. At the end of the blog post, there is a simple exercise that you will need to solve to portray some understanding of the way that &lt;mark&gt;ViTs&lt;/mark&gt; and the interpretability methods operate.">


    <link rel="stylesheet" href="/assets/css/codeblock.scss">
    <link rel="stylesheet" href="/assets/css/codeblock.scss">

    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.3/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/.css" media="" id="highlight_theme_light">



    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%98%8E&lt;/text&gt;&lt;/svg&gt;">
    
    

    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://kristosh.github.io//blog/2023/Explainable-ViT/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

    <link rel="stylesheet" href="/assets/css/codeblock.scss">

    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "How to explain the behavior of vision transformers?",
      "description": "This page's goal is to present techniques that can shed light on how Vision Transformers' models (<mark>ViTs</mark>) operate. We will first have a refresher on the ViTs and how they work. We will introduce a simple example of a ViT classifier trained on a pizza-sushi-steak dataset and use a pre-trained model to efficiently classify the images. Then, we will introduce various methods to visualize the way that they function. These approaches range from visualizing the attention maps to visualizing the query/key and value, but also using the backpropagated gradient similar to <mark>gradCAM</mark> algorithm. We will make use of <mark>PyTorch</mark> implementation to demonstrate some of these techniques. At the end of the blog post, there is a simple exercise that you will need to solve to portray some understanding of the way that <mark>ViTs</mark> and the interpretability methods operate.",
      "published": "May 13, 2023",
      "authors": [
        {
          "author": "Christos Athanasiadis",
          "authorURL": "https://www.linkedin.com/in/christos-athanasiadis-a3b51035/",
          "affiliations": [
            {
              "name": "UvA, Interpretability and Explainability in AI",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Christos¬†</span>Athanasiadis</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>How to explain the behavior of vision transformers?</h1>
        <p>This page's goal is to present techniques that can shed light on how Vision Transformers' models (<mark>ViTs</mark>) operate. We will first have a refresher on the ViTs and how they work. We will introduce a simple example of a ViT classifier trained on a pizza-sushi-steak dataset and use a pre-trained model to efficiently classify the images. Then, we will introduce various methods to visualize the way that they function. These approaches range from visualizing the attention maps to visualizing the query/key and value, but also using the backpropagated gradient similar to <mark>gradCAM</mark> algorithm. We will make use of <mark>PyTorch</mark> implementation to demonstrate some of these techniques. At the end of the blog post, there is a simple exercise that you will need to solve to portray some understanding of the way that <mark>ViTs</mark> and the interpretability methods operate.</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#vision-transformer-vit">Vision Transformer ViT</a></div>
            <div><a href="#pizza-sushi-steak-classifier">Pizza-sushi-steak üçïüç£ü•© classifier</a></div>
            <div><a href="#explainable-vit">Explainable ViT</a></div>
            <div><a href="#tosubmit">TOSUBMIT</a></div>
            <div><a href="#conclusions">Conclusions</a></div>
            <div><a href="#references">References</a></div>
            
          </nav>
        </d-contents>

        <h1 id="introduction">Introduction</h1>

<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2023-06-23-Explainable-ViT/ViT_architecture.PNG" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

<p>What is a Vision Transformer? What is going on with the inner mechanism of it? How do they even work? Can we poke at them and dissect them into pieces to understand them better?</p>

<p>These are some questions that we will try to answer in this post. Firstly, we will try to remind our reader about what is exactly a vision transformer and how it works. We will develop a simple image classifier that distinguishes between üçïüç£ü•© images. Hence, we will try to revise methods that aim to shed light on their inner mechanisms. Our aim goal is to investigate all the current standard practices for visualizing the mechanisms that cause the <code class="language-plaintext highlighter-rouge">ViT</code> classifier to predict a specific class each time. These visualizations could be useful for:</p>

<ul>
  <li>
    <p><strong>the reseracher/developer</strong>: Which parts of the transformers are activated when we input a specific image? Being able to look at intermediate activation layers different heads and part of the architecture and investigate what led to specific model activation.</p>
  </li>
  <li>
    <p><strong>the reseracher/developer</strong>: What did it learn? What type of patterns did the model learn? Usually, this is in the form of the question <em>What input image maximizes the response from this activation?</em>, and you can use variants of <em>Activation Maximization</em> for that.</p>
  </li>
  <li>
    <p><strong>both the developer and the user</strong>: What did it see in this image? Being able to Answer <em>What part of the image is responsible for the network prediction</em>, is sometimes called <em>feature or pixel attribution</em>.</p>
  </li>
</ul>

<p>This tutorial was based on the code from the following tutorial: <a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformer.html" rel="external nofollow noopener" target="_blank">https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformer.html</a>.</p>

<p>At the end of this tutorial a simple TODO exercise will be provided to gauge the performance of different XAI methods for our built <code class="language-plaintext highlighter-rouge">ViT</code> model.</p>

<h1 id="vision-transformer-vit">Vision Transformer ViT</h1>

<p>The vanilla Transformer architecture was introduced by Vaswani et al. in 2017 [1], to tackle sequential data and particularly textual information for the machine translation task. Given the success of the Transformer in the NLP domain, Dosovitskiy et al. [2] proposed the Vision Transformer (<code class="language-plaintext highlighter-rouge">ViT</code>) architecture for visual classification tasks. The <code class="language-plaintext highlighter-rouge">ViT</code> architecture is the standard transformer architecture but with visual information as input instead. In the <code class="language-plaintext highlighter-rouge">ViT</code> context, we need to convert the <code class="language-plaintext highlighter-rouge">3D</code> grid of pixels into a sequence of token embeddings. This could be done by splitting the image into non-overlapping patches and then, each patch should be flattened into a <code class="language-plaintext highlighter-rouge">1D</code> vector and then linearly projected into a vector of token embeddings. Finally, these token embeddings are fed into the <code class="language-plaintext highlighter-rouge">ViT</code> architecture in a similar way as the vanilla transformers. The basic blocks of the <code class="language-plaintext highlighter-rouge">ViT</code> architecture can be seen in the previous image and are:</p>

<ul>
  <li><strong>inputs</strong></li>
  <li><strong>linear projection (embedding layer)</strong></li>
  <li><strong>positional encoding</strong></li>
  <li>
<strong>transformer encoder</strong>
    <ul>
      <li><strong>Layer normalization</strong></li>
      <li><strong>multi-head self-attention</strong></li>
      <li><strong>feed-forward neural network</strong></li>
    </ul>
  </li>
  <li><strong>classification token</strong></li>
  <li><strong>MLP head</strong></li>
  <li><strong>output predictions</strong></li>
</ul>

<p>To help the reader comprehend all the above, we will provide a simple grouping of definitions and use the following two terms:</p>

<ul>
  <li>
<strong>layers</strong>: are basic elements that are used to build the architecture. For example, the multi-head self-attention can be considered as a layer. It‚Äôs important to mention that a Transformer is composed usually of several Encoders. Each of these Enconders can be referred to in the literature as a layer as well.</li>
  <li>
<strong>blocks</strong>: a grouping of layers (for instance the whole encoder can be seen as a block of layers).</li>
</ul>

<p>The ViTs architecture is comprised of several stages:</p>

<ul>
  <li>
<strong>Patch + Position Embedding (inputs)</strong>: Turns the input image into a sequence of image patches and adds a position number to specify in what order the patch comes in.</li>
  <li>
<strong>Linear projection of flattened patches (Embedded Patches)</strong>: The image patches are flattened and then projected into embeddings. The benefit of using embeddings rather than just the image values is that embeddings are learnable representations of the image that can improve with training.</li>
  <li>
<strong>Norm</strong>: This is short for <code class="language-plaintext highlighter-rouge">Layer Normalization</code> or <code class="language-plaintext highlighter-rouge">LayerNorm</code>, a technique for regularizing (reducing overfitting) a neural network, you can use LayerNorm via the <code class="language-plaintext highlighter-rouge">PyTorch</code> layer <code class="language-plaintext highlighter-rouge">torch.nn.LayerNorm()</code>.</li>
  <li>
<strong>Multi-Head Attention</strong>: This is a Multi-Headed Self-Attention layer or <code class="language-plaintext highlighter-rouge">MSA</code> for short. You can create an MSA layer via the PyTorch layer <code class="language-plaintext highlighter-rouge">torch.nn.MultiheadAttention()</code>.</li>
  <li>
<strong>MLP (or Multilayer perceptron)</strong>: An MLP can often refer to any collection of feedforward layers (or in PyTorch‚Äôs case, a collection of layers with a forward() method). In the <code class="language-plaintext highlighter-rouge">ViT</code> Paper, the authors refer to the MLP as ‚ÄúMLP block‚Äù and it contains two <code class="language-plaintext highlighter-rouge">torch.nn.Linear()</code> layers with a <code class="language-plaintext highlighter-rouge">torch.nn.GELU()</code> non-linearity activation in between them (section 3.1) and a <code class="language-plaintext highlighter-rouge">torch.nn.Dropout()</code> layer after each.</li>
  <li>
<strong>Transformer Encoder</strong>: The Transformer Encoder, is a collection of the layers listed above. There are two skip connections inside the Transformer encoder (the ‚Äú+‚Äù symbols) meaning the layer‚Äôs inputs are fed directly to immediate layers as well as subsequent layers. The overall <code class="language-plaintext highlighter-rouge">ViT</code> architecture is comprised of several Transformer encoders stacked on top of each other.</li>
  <li>
<strong>MLP Head</strong>: This is the output layer of the architecture, it converts the learned features of an input to a class output. Since we‚Äôre working on image classification, you could also call this the ‚Äúclassifier head‚Äù. The structure of the <code class="language-plaintext highlighter-rouge">MLP</code> Head is similar to the MLP block.</li>
</ul>

<p>Breaking the hyperparameters down:</p>

<ul>
  <li>
<strong>Layers</strong>: How many Transformer Encoder blocks are there? (each of these will contain an MSA block and the MLP block)</li>
  <li>
<strong>Hidden size  D</strong>: This is the embedding dimension throughout the architecture, this will be the size of the vector that our image gets turned into when it gets patched and embedded. Generally, the larger the embedding dimension, the more information can be captured, which leads to better results. However, a larger embedding comes at the cost of more computing. One niche issue here is that we need to distinguish the <em>layers</em> as hyperparameters that refer to the number of Transformer Encoder blocks and the <em>layer</em> as a building block of the encoder.</li>
  <li>
<strong>MLP size</strong>: What are the number of hidden units in the MLP layers?</li>
  <li>
<strong>Heads</strong>: How many heads are there in the Multi-Head Attention layers?
<!-- - **Params**: What is the total number of parameters of the model? Generally, more parameters lead to better performance but at the cost of more compute. You'll notice even ViT-Base has far more parameters than any other model we've used so far. -->
</li>
</ul>

<h2 id="vision-transformers-vits-tokenization">Vision Transformers (ViTs) Tokenization</h2>

<p>The standard Transformer receives as input a 1D sequence of token embeddings. To handle 3D images, we reshape the image $\mathbf{x} \in \mathbb{R}^{H \times W \times C}$ into a sequence of flattened patches with size $\mathbf{x}_P \in \mathbb{R}^{N \times CP^2}$, when $H, W$ represent the height and the width of an image while $C$ represents the number of channels, then, $N$ is the number of patches and $P$ is the patch dimensionality. The Transformer uses constant latent vector size  $D$ through all of its layers, so we flatten the patches and map to $D$ dimensions with a trainable linear projection (Equation 1 from the <code class="language-plaintext highlighter-rouge">ViT</code> paper). We refer to the output of this projection as the patch embeddings. If the input image is of size $224 \times 224 \times 3$ and the patch size is $16$ then the output should be of size $196 \times 768$, where the first dimension is the number of patches and the second dimension is the size of the patch embeddings $16\cdot 16\cdot 3 = 768$.</p>

<!-- - is is the same with the transformer enconder? -->
<h2 id="transformer-enconder">Transformer enconder</h2>

<p>After having created the patches, we should proceed with the implementation of the transformer encoder which can be seen in Figure 1. It can mainly divided into the <code class="language-plaintext highlighter-rouge">multi-head attention</code> (MSA) and the <code class="language-plaintext highlighter-rouge">MLP</code> layer. The multi-head self-attention mechanism is used to capture the dependencies between the patches. The feed-forward neural network is used to capture the non-linear interactions between the patches. The following image portrays the mechanism of the attention block.</p>

<p>Here we will need to decide whether the input to our model will be the full image or the image patches. To decide that we will take into account that a lot of pre-trained models have as input the full image. Thus, for now, we will use the full image as input to our model.</p>

<p>Moreover, note that we make use also of <code class="language-plaintext highlighter-rouge">Layer normalization</code> ‚Ä¶</p>

<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2023-06-23-Explainable-ViT/ViT_architecture.PNG" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

<h2 id="layer-normalization">Layer normalization</h2>

<p>The LayerNorm (LN) Layer Normalization (<code class="language-plaintext highlighter-rouge">torch.nn.LayerNorm()</code> or <code class="language-plaintext highlighter-rouge">Norm</code> or <code class="language-plaintext highlighter-rouge">LayerNorm</code> or <code class="language-plaintext highlighter-rouge">LN</code>) normalizes an input over the last dimension. PyTorch‚Äôs <code class="language-plaintext highlighter-rouge">torch.nn.LayerNorm()</code> main parameter is normalized_shape which we can set to be equal to the dimension size we‚Äôd like to normalize over (in our case it‚Äôll be  $D$ or $768$ for <code class="language-plaintext highlighter-rouge">ViT</code>-Base). What does it do?</p>

<p>Layer Normalization helps improve training time and model generalization (ability to adapt to unseen data). I like to think of any kind of normalization as ‚Äúgetting the data into a similar format‚Äù or ‚Äúgetting data samples into a similar distribution‚Äù. Imagine trying to walk up (or down) a set of stairs all with differing heights and lengths.</p>

<p>It‚Äôd take some adjustment on each step, right? And what you learn for each step wouldn‚Äôt necessarily help with the next one since they all differ, increasing the time it takes you to navigate the stairs. Normalization (including <code class="language-plaintext highlighter-rouge">Layer Normalization</code>) is the equivalent of making all the stairs the same height and length except the stairs are your data samples. So just like you can walk up (or down) stairs with similar heights and lengths much easier than those with unequal heights and widths, neural networks can optimize over data samples with similar distributions (similar mean and standard deviations) easier than those with varying distributions.</p>

<h1 id="pizza-sushi-steak--classifier">Pizza-sushi-steak üçïüç£ü•© classifier</h1>

<p>Now it‚Äôs time to code all the above into a classifier. We will implement a simple <code class="language-plaintext highlighter-rouge">ViT</code> classifier for the pizza-sushi-steak dataset. The dataset contains train and test folders with $450$ images for training and 150 images for testing. We will start by providing some code for setting up our data. Firstly, we should download the dataset:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td>
<td class="rouge-code"><pre><span class="n">image_path</span> <span class="o">=</span> <span class="nf">download_data</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="sh">"</span><span class="s">https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">destination</span><span class="o">=</span><span class="sh">"</span><span class="s">pizza_steak_sushi</span><span class="sh">"</span><span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<p>Set up the paths for the training and testing data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td>
<td class="rouge-code"><pre><span class="n">data_path</span> <span class="o">=</span> <span class="nc">Path</span><span class="p">(</span><span class="sh">"</span><span class="s">data/</span><span class="sh">"</span><span class="p">)</span>
<span class="n">image_path</span> <span class="o">=</span> <span class="n">data_path</span> <span class="o">/</span> <span class="sh">"</span><span class="s">pizza_steak_sushi</span><span class="sh">"</span>

<span class="c1"># Setup directory paths to train and test images
</span><span class="n">train_dir</span> <span class="o">=</span> <span class="n">image_path</span> <span class="o">/</span> <span class="sh">"</span><span class="s">train</span><span class="sh">"</span>
<span class="n">test_dir</span> <span class="o">=</span> <span class="n">image_path</span> <span class="o">/</span> <span class="sh">"</span><span class="s">test</span><span class="sh">"</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<p>Then, we would like to perform some basic transformations to our data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td>
<td class="rouge-code"><pre><span class="c1"># Create image size (from Table 3 in the ViT paper)
</span><span class="n">IMG_SIZE</span> <span class="o">=</span> <span class="mi">224</span>

<span class="c1"># Create transform pipeline manually
</span><span class="n">manual_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">Resize</span><span class="p">((</span><span class="n">IMG_SIZE</span><span class="p">,</span> <span class="n">IMG_SIZE</span><span class="p">)),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
<span class="p">])</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<p>Then you will need to create the necessary dataloaders for train and test sets:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre></td>
<td class="rouge-code"><pre><span class="c1"># Use ImageFolder to create dataset(s)
</span>  <span class="n">train_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">ImageFolder</span><span class="p">(</span><span class="n">train_dir</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
  <span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">ImageFolder</span><span class="p">(</span><span class="n">test_dir</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>

  <span class="c1"># Get class names
</span>  <span class="n">class_names</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">.</span><span class="n">classes</span>

  <span class="c1"># Turn images into data loaders
</span>  <span class="n">train_dataloader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span>
      <span class="n">train_data</span><span class="p">,</span>
      <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
      <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
      <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
      <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
  <span class="p">)</span>
  
  <span class="n">test_dataloader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span>
      <span class="n">test_data</span><span class="p">,</span>
      <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
      <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="c1"># don't need to shuffle test data
</span>      <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
      <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
  <span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<p>Check the repository (here) for the implementations for the Dataloaders. As a follow-up step, we should return a batch of images and labels with the following code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
</pre></td>
<td class="rouge-code"><pre><span class="n">image_batch</span><span class="p">,</span> <span class="n">label_batch</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="nf">iter</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">))</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>
<h1 id="create-a-vit-classifier">Create a ViT classifier</h1>

<p>Having loaded the data, now it‚Äôs time to introduce a simple <code class="language-plaintext highlighter-rouge">ViT</code> model and fit our data. We will call the <code class="language-plaintext highlighter-rouge">ViT</code> model and pass the image batch to it. The output of the model will be the logits for each class. We will then use the cross-entropy loss to calculate the loss and the Adam optimizer to update the weights of the model. The following code will help you to create a simple <code class="language-plaintext highlighter-rouge">ViT</code> classifier:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td>
<td class="rouge-code"><pre><span class="n">device</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="nf">set_seeds</span><span class="p">()</span>
<span class="c1"># Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi)
</span><span class="n">vit</span> <span class="o">=</span> <span class="nc">ViT</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">cls_names</span><span class="p">))</span>

<span class="c1"># Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">vit</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">3e-3</span><span class="p">,</span> <span class="c1"># Base LR from Table 3 for ViT-* ImageNet-1k
</span>    <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="c1"># default values but also mentioned in ViT paper section 4.1 (Training &amp; Fine-tuning)
</span>    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span> <span class="c1"># from the ViT paper section 4.1 (Training &amp; Fine-tuning) and Table 3 for ViT-* ImageNet-1k
</span>
<span class="c1"># Setup the loss function for multi-class classification
</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="c1"># Train the model and save the training results to a dictionary
</span><span class="n">results</span> <span class="o">=</span> <span class="nf">train_function</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">vit</span><span class="p">,</span>
    <span class="n">train_dataloader</span><span class="o">=</span><span class="n">train_dataloader</span><span class="p">,</span>
    <span class="n">test_dataloader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<h2 id="building-the-vit-model">Building the ViT model</h2>

<p>Of course, we will need to develop the code for the <code class="language-plaintext highlighter-rouge">ViT</code> model as well. That is a bit more complicated. At first, we will illustrate the whole code and then, we will analyze it step by step. The code looks as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
</pre></td>
<td class="rouge-code"><pre><span class="c1"># 1. Create a ViT class that inherits from nn.Module
</span><span class="k">class</span> <span class="nc">ViT</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.</span><span class="sh">"""</span>
    <span class="c1"># Initialize the class with hyperparameters from Table 1 and Table 3 from original ViT paper
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>
            <span class="n">img_size</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="c1"># Training resolution from Table 3 in ViT paper
</span>            <span class="n">in_channels</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="c1"># Number of channels in input image
</span>            <span class="n">patch_size</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="c1"># Patch size
</span>            <span class="n">num_transformer_layers</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="c1"># Layers from Table 1 for ViT-paper
</span>            <span class="n">embedding_dim</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="c1"># Hidden size D from Table 1 for ViT-paper
</span>            <span class="n">mlp_size</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span> <span class="c1"># MLP size from Table 1 for ViT-paper
</span>            <span class="n">num_heads</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="c1"># Heads from Table 1 for ViT-paper
</span>            <span class="n">attn_dropout</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">mlp_dropout</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
            <span class="n">embedding_dropout</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
            <span class="n">num_classes</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span> <span class="c1"># The nubmer of classes in the dataset
</span>        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span> <span class="c1"># inherited initialization from nn.Module
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="p">(</span><span class="n">img_size</span> <span class="o">*</span> <span class="n">img_size</span><span class="p">)</span> <span class="o">//</span> <span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span> <span class="c1"># Calculate number of patches (height * width/patch^2) 
</span>        <span class="n">self</span><span class="p">.</span><span class="n">class_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">),</span> <span class="c1"># Create learnable class embedding
</span>            <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">position_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_patches</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">),</span>  <span class="c1"># Create learnable position embedding
</span>            <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">embedding_dropout</span><span class="p">)</span>  <span class="c1"># Create embedding dropout value
</span>        <span class="n">self</span><span class="p">.</span><span class="n">patch_embedding</span> <span class="o">=</span> <span class="nc">PatchEmbedding</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="c1"># Create patch embedding layer
</span>            <span class="n">patch_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
            <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">)</span>

        <span class="c1"># 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())
</span>        <span class="c1"># Note: The "*" means "all"
</span>        <span class="n">self</span><span class="p">.</span><span class="n">transformer_encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="nc">TransformerEncoderBlock</span><span class="p">(</span><span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">mlp_size</span><span class="o">=</span><span class="n">mlp_size</span><span class="p">,</span>
            <span class="n">mlp_dropout</span><span class="o">=</span><span class="n">mlp_dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_transformer_layers</span><span class="p">)])</span>

        <span class="c1"># 10. Create classifier head
</span>        <span class="n">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
                      <span class="n">out_features</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># 11. Create a forward() method
</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Get batch size
</span>        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># Create class token embedding and expand it to match the batch size (equation 1)
</span>        <span class="n">class_token</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">class_embedding</span><span class="p">.</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># "-1" means to infer the dimension (try this line on its own)
</span>        <span class="c1"># Create patch embedding (equation 1)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">patch_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Concat class embedding and patch embedding (equation 1)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">class_token</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Add position embedding to patch embedding (equation 1)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">position_embedding</span> <span class="o">+</span> <span class="n">x</span>
        <span class="c1"># Run embedding dropout (Appendix B.1)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding_dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Pass patch, position and class embedding through transformer encoder layers (equations 2 &amp; 3)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transformer_encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Put 0 index logit through classifier (equation 4)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="c1"># run on each sample in a batch at 0 index
</span>        <span class="k">return</span> <span class="n">x</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>
<h3 id="breaking-down-the-code-step-by-step">Breaking down the code step-by-step</h3>

<p>Firstly, as mentioned in the code block comments we follow the details of the ViT paper such as <code class="language-plaintext highlighter-rouge">batch size</code>, <code class="language-plaintext highlighter-rouge">number of patches</code>, <code class="language-plaintext highlighter-rouge">number of layers</code>, the <code class="language-plaintext highlighter-rouge">dimensionality of the embeddings</code>, <code class="language-plaintext highlighter-rouge">number of heads</code>, etc. More details can be found in the paper‚Äôs Table 1 and Table 3.</p>

<p>The first thing that our code tries to emulate is the creation of patches. Given, an image we create patches of size $16 \times 16$ ($P \times P$). Thus, if the input image has size $H \times W \times C$  and is $224 \times 224 \times 3$, the total amount of patches is $N = 196$, and can be calculated by the following formula $N = HW/P^{2}$. Then, these image patches are turned into embeddings, by using the <code class="language-plaintext highlighter-rouge">PatchEmbedding</code> functionality. The benefit of turning the raw images into embeddings is that we can learn a representation of the image that can improve with training.</p>

<p>Different values for the size of the embeddings can be found in Table 1, but throughout this tutorial, we will make use of $D = 768$. The idea is to first split the image into patches and then apply a learnable 2d convolutional layer to each patch. If we set the proper values for the kernel_size and stride parameters of a <code class="language-plaintext highlighter-rouge">torch.nn.Conv2d()</code> then we can have the desired output embedding, for instance, $D = 768$ in our case. To facilitate the dimensions of output smoothly we will need to make use of a <code class="language-plaintext highlighter-rouge">flatten()</code> function to flatten the output of the convolutional layer.</p>

<p>The next step is to stack $m$ Transformer Encoders together using the following code: <code class="language-plaintext highlighter-rouge">nn.Sequential(*[TransformerEncoderBlock(.)</code> and finally add a linear layer that will output the desired amount of classes <code class="language-plaintext highlighter-rouge">nn.Linear(in_features=embedding_dim, out_features=num_classes)</code>.</p>

<p><strong>PatchEmbedding code</strong>: After having created the patches in the main <code class="language-plaintext highlighter-rouge">ViT</code> class, the next step is to calculate the embeddings of the patches. This is done by the <code class="language-plaintext highlighter-rouge">PatchEmbedding</code> class. The code for the <code class="language-plaintext highlighter-rouge">PatchEmbedding</code> class is as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre></td>
<td class="rouge-code"><pre><span class="k">class</span> <span class="nc">PatchEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Turns a 2D input image into a 1D sequence learnable embedding vector.

    Args:
        in_channels (int): Number of color channels for the input images. Defaults to 3.
        patch_size (int): Size of patches to convert input image into. Defaults to 16.
        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.
    </span><span class="sh">"""</span>
    <span class="c1"># 2. Initialize the class with appropriate variables
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">768</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">patcher</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># only flatten the feature map dimensions into a single vector
</span>            
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Create assertion to check that inputs are the correct shape
</span>        <span class="n">image_resolution</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Perform the forward pass
</span>        <span class="n">x_patched</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">patcher</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x_flattened</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="n">x_patched</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_flattened</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># adjust so the embedding is on the final dimension [batch_size, P^2‚Ä¢C, N] -&gt; [batch_size, N, P^2‚Ä¢C]
</span></pre></td>
</tr></tbody></table></code></pre></div></div>

<p><strong>TransformerEncoderBlock code</strong>: The second main part of the code is the <code class="language-plaintext highlighter-rouge">TransformerEncoderBlock</code> class. This class is responsible for the creation of the Transformer Encoder block. It is mainly composed as Figure 1 portrays in two parts: <code class="language-plaintext highlighter-rouge">MultiheadSelfAttentionBlock</code> and <code class="language-plaintext highlighter-rouge">MLPBlock</code> blocks. The code for the <code class="language-plaintext highlighter-rouge">TransformerEncoderBlock</code> class is as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td>
<td class="rouge-code"><pre><span class="k">class</span> <span class="nc">TransformerEncoderBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Creates a Transformer Encoder block.</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>
            <span class="n">embedding_dim</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> 
            <span class="n">num_heads</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> 
            <span class="n">mlp_size</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span> 
            <span class="n">mlp_dropout</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
            <span class="n">attn_dropout</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span> 
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">msa_block</span> <span class="o">=</span> <span class="nc">MultiheadSelfAttentionBlock</span><span class="p">(</span><span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">attn_dropout</span><span class="o">=</span><span class="n">attn_dropout</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">mlp_block</span> <span class="o">=</span> <span class="nc">MLPBlock</span><span class="p">(</span><span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="c1"># You can find more information for this part of the code in the repository
</span>            <span class="n">mlp_size</span><span class="o">=</span><span class="n">mlp_size</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">mlp_dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span>  <span class="n">self</span><span class="p">.</span><span class="nf">msa_block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mlp_block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<h2 id="training-function-for-the-vit-model">Training function for the ViT model</h2>

<p>Having defined already the dataset, our model and the loss function, we can directly proceed with the training of our <code class="language-plaintext highlighter-rouge">ViT</code> model. The idea is to iterate through all the epochs and batches and update the parameters of the model using backpropagation as usual. Then, we should report the loss and the accuracy of the model for the training and test sets. The code should look as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td>
<td class="rouge-code"><pre><span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="mi">0</span>    
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
        <span class="n">X</span> <span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span> 
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="n">y_pred_class</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">train_acc</span> <span class="o">+=</span> <span class="p">(</span><span class="n">y_pred_class</span> <span class="o">==</span> <span class="n">y</span><span class="p">).</span><span class="nf">sum</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>

<span class="n">train_loss</span> <span class="o">=</span> <span class="n">train_loss</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
<span class="n">train_acc</span> <span class="o">=</span> <span class="n">train_acc</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<p>Of course, we will need to measure also the performance in the test set as usual. The code is identical to the training process and can be found in the repository.</p>

<h2 id="measuring-the-performance-of-the-vit-model">Measuring the performance of the ViT model</h2>

<p>After having trained the model, we should report the performance of the model in the test set. The code should look as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
</pre></td>
<td class="rouge-code"><pre><span class="n">weights_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">models/pre_trained_vit_sushi_768_v2.pth</span><span class="sh">"</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">weights_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span> <span class="n">device</span><span class="p">)</span>
<span class="n">pretrained_vit</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">pretrained_vit</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>

<span class="k">if</span> <span class="n">transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">image_transform</span> <span class="o">=</span> <span class="n">transform</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">image_transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="p">.</span><span class="nc">Resize</span><span class="p">(</span><span class="n">image_size</span><span class="p">),</span>
        <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
            <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]),])</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">inference_mode</span><span class="p">():</span>
    <span class="n">transformed_image</span> <span class="o">=</span> <span class="nf">image_transform</span><span class="p">(</span><span class="n">img</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">target_image_pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">transformed_image</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

<span class="n">target_image_pred_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">target_image_pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">target_image_pred_label</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">target_image_pred_probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Plot the results
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Pred: </span><span class="si">{</span><span class="n">class_names</span><span class="p">[</span><span class="n">target_image_pred_label</span><span class="p">]</span><span class="si">}</span><span class="s"> | Prob: </span><span class="si">{</span><span class="n">target_image_pred_probs</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<p>Of course, you can also measure the performance of the model using the test set and extract performance metrics such as accuracy, precision, recall, and $F_1$-score. You can make use of Tensoboard to visualize the results as well.</p>

<h2 id="loading-a-pre-trained-vit-model">Loading a pre-trained ViT model</h2>

<p>As we saw in the previous section, it is not possible to train our model with only that small amount of data. Thus, we will try to perform instead Transfer learning to load pre-trained weights on <code class="language-plaintext highlighter-rouge">ImageNet</code> using the <code class="language-plaintext highlighter-rouge">ViT_B_16_Weights</code> model that comes with the <code class="language-plaintext highlighter-rouge">torchvision</code> package. Of course, this model is trained for a different target than our desired target üçïüç£ü•©. Thus, we will need to change the layers that relate to the class and replace the output with the desired amount of output layers. We will need also to freeze all the rest layers:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td>
<td class="rouge-code"><pre><span class="c1"># Load the pre-trained ViT model        
</span><span class="n">retrained_vit_weights</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">ViT_B_16_Weights</span><span class="p">.</span><span class="n">DEFAULT</span> <span class="c1"># requires torchvision &gt;= 0.13, "DEFAULT" means best available
</span><span class="n">pretrained_vit</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nf">vit_b_16</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">retrained_vit_weights</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">pretrained_vit</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
    <span class="n">parameter</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>

<span class="n">pretrained_vit</span><span class="p">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">)).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<p>Then, we can perform the training process as usual and report the results. Note that you could make use of the pre-trained weights of your preference, however, you should be a bit careful about the parameters that need to be updated. For instance, some pre-trained weights do not follow the same hyper-parameters as in the case of the original <code class="language-plaintext highlighter-rouge">ViT</code> paper.</p>

<h1 id="explainable-vit">Explainable ViT</h1>

<p>Until we managed to successfully train a <code class="language-plaintext highlighter-rouge">ViT</code> using the images in our handmade dataset of üçïüç£ü•© images using transfer learning. We measure the performance in a small test set and visualize the results. But how exactly does the model classify each specific image? Which parts of the model activated and led to a specific decision? Which layers are responsible for that decision?</p>

<p>One simple way to investigate the inner mechanisms of the <code class="language-plaintext highlighter-rouge">ViT</code> model is to visualize the attention weights which is the easiest and most popular approach to interpret a model‚Äôs decisions and to gain insights about its internals. These weights are calculated by the <code class="language-plaintext highlighter-rouge">Multi-Head Self-Attention</code> mechanism and can help us to understand which parts of the image are responsible for the decision of the model. Now, the question that pops up is: which attention maps are we going to visualize? From which layer? Which head? Remember that our model is composed of several layers and each layer (in particular we chose <code class="language-plaintext highlighter-rouge">12</code> layers).</p>

<p>Transformer model, in each layer, self-attention combines information from attended embeddings of the previous layer to compute new embeddings for each token. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed for a more thorough discussion on how the identity of tokens gets less and less represented in the embedding of that position as we go into deeper layers.</p>

<p>Hence, when looking at the $i$th self-attention layer, we can not interpret the attention weights as the attention to the input tokens, i.e., embeddings in the input layer. This makes attention weights unreliable as explanation probes to answer questions like ‚ÄúWhich part of the input is the most important when generating the output?‚Äù (except for the very first layer where the self-attention is directly applied to the input tokens.)</p>

<p>Take home message: across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes.</p>

<p>We can start by visualizing the attention maps of one of these layers. However, this approach is not class-specific and we end up ignoring most of the attention scores. Moreover, other layers are not even considered. Somehow a more sophisticated approach to take into account all the layers is needed here.</p>

<h2 id="attention-rollout">Attention Rollout</h2>

<p>At every Transformer block, we get an attention Matrix $A_{ij}$ that defines how much attention is going to flow from image patch (token) $j$ in the previous layer to image patch (token) $i$ in the next layer. We can multiply the Matrices between every two layers, to get the total attention flow between them. Why?</p>

<p>Attention rollout and attention flow recursively compute the token attention in each layer of a given model given the embedding attention as input.
They differ in the assumptions they make about how attention weights in lower layers affect the flow of information to the higher layers and
whether to compute the token attention relative to each other or independently.</p>

<p>When we only use attention weights to approximate the flow of information in Transformers, we ignore the residual connections We can model them by adding the identity matrix $\mathbb{I}$ to the layer Attention matrices: $A_{ij}+\mathbb{I}$. We have multiple attention heads. What do we do about them? The Attention rollout paper suggests taking the average of the heads. As we will see, it can make sense using other choices: like the minimum, the maximum, or using different weights. Finally, we get a way to recursively compute the Attention Rollout matrix at layer L:</p>

\[AttentionRollout_{L}=(A_L+\mathbb{I}) AttentionRollout_{L‚àí1}\]

<p>We also have to normalize the rows, to keep the total attention flow 1.</p>

<p>Regarding the implementation of this method, the main code for implementing the <code class="language-plaintext highlighter-rouge">Attention Rollout</code> method is as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre></td>
<td class="rouge-code"><pre>
<span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">attentions</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">attention</span> <span class="ow">in</span> <span class="n">attentions</span><span class="p">:</span>
        
        <span class="c1"># fusion methods 
</span>        <span class="c1">#TODO implementation
</span>
        <span class="n">flat</span> <span class="o">=</span> <span class="n">attention_heads_fused</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">attention_heads_fused</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># a list with the fused attention heads for each layer
</span>        <span class="n">_</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">flat</span><span class="p">.</span><span class="nf">topk</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">flat</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">discard_ratio</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">indices</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">flat</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">indices</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">I</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">attention_heads_fused</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># identity matrix
</span>        <span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="n">attention_heads_fused</span> <span class="o">+</span> <span class="mf">1.0</span><span class="o">*</span><span class="n">I</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span> <span class="c1"># take into account the residual connections
</span>        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">/</span> <span class="n">a</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># normalize the rows
</span>        
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">attention_heads_fused</span><span class="p">)</span> <span class="c1"># the attention rollout matrix for each layer
</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span> <span class="p">,</span> <span class="mi">1</span> <span class="p">:]</span> <span class="c1"># Look at the total attention between the class token and the image patches
</span>    <span class="c1"># In case of 224x224 image, this brings us from 196 to 14
</span>    <span class="n">width</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">mask</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">width</span><span class="p">).</span><span class="nf">numpy</span><span class="p">()</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mask</span> 
</pre></td>
</tr></tbody></table></code></pre></div></div>

<p>where <code class="language-plaintext highlighter-rouge">discard_ratio</code> is a hyperparameter and the variable <code class="language-plaintext highlighter-rouge">attention_heads_fused</code> represents the way that we fused the attention heads. That occurs by averaging or keeping the <code class="language-plaintext highlighter-rouge">max</code> and <code class="language-plaintext highlighter-rouge">min</code> for the attention maps.</p>

<h3 id="cons-of-this-method">Cons of this method</h3>

<ul>
  <li>This methodology is not class-specific</li>
  <li>They end up ignoring most of the attention scores, and other layers are not even considered.</li>
  <li>
    <h2 id="gradient-attention-rollout">Gradient Attention Rollout</h2>
  </li>
</ul>

<p>The Attention that flows in the transformer passes along information belonging to different classes. Gradient rollout lets us see what locations the network paid attention too, but it tells us nothing about if it ended up using those locations for the final classification.</p>

<p>We can multiply the attention with the gradient of the target class output, and take the average among the attention heads (while masking out negative attentions) to keep only attention that contributes to the target category (or categories).</p>

<p>When fusing the attention heads in every layer, we could just weight all the attentions (in the current implementation it‚Äôs the attentions after the softmax, but maybe it makes sense to change that) by the target class gradient, and then take the average among the attention heads</p>

<p>The main code for implementing the <code class="language-plaintext highlighter-rouge">Gradient Attention Rollout</code> method is as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre></td>
<td class="rouge-code"><pre><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">attention</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">attentions</span><span class="p">,</span> <span class="n">gradients</span><span class="p">):</span>                
            <span class="k">if</span> <span class="n">counter</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">continue</span>
        
            <span class="n">weights</span> <span class="o">=</span> <span class="n">grad</span>            
            <span class="n">attention_heads_fused</span> <span class="o">=</span> <span class="p">(</span><span class="n">attention</span><span class="o">*</span><span class="n">weights</span><span class="p">).</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">attention_heads_fused</span><span class="p">[</span><span class="n">attention_heads_fused</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="n">pdb</span><span class="p">.</span><span class="nf">set_trace</span><span class="p">()</span>
            <span class="c1"># Drop the lowest attentions, but
</span>            <span class="c1"># don't drop the class token
</span>            <span class="n">flat</span> <span class="o">=</span> <span class="n">attention_heads_fused</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">attention_heads_fused</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">flat</span><span class="p">.</span><span class="nf">topk</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">flat</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">discard_ratio</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
            <span class="c1">#indices = indices[indices != 0]
</span>            <span class="n">flat</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">indices</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="n">I</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">attention_heads_fused</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
            <span class="n">a1</span> <span class="o">=</span> <span class="p">(</span><span class="n">attention_heads_fused</span> <span class="o">+</span> <span class="mf">1.0</span><span class="o">*</span><span class="n">I</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
            <span class="n">a1</span> <span class="o">=</span> <span class="n">a1</span> <span class="o">/</span> <span class="n">a1</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1">#pdb.set_trace()
</span>            <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<h1 id="todo">TODO</h1>
<p>So far we have presented two simple methods for explainable <code class="language-plaintext highlighter-rouge">ViT</code> based on attention maps and the gradient. We have tested these methods using single images for visualization purposes from the üçïüç£ü•© dataset. However, we haven‚Äôt yet introduced any quantified way to measure the performance of these methodologies. As a simple <code class="language-plaintext highlighter-rouge">TODO</code> you will need to come up with ways to measure the performance of these two methodologies. You will need to find a ground truth and compare both methodologies.</p>

<p>Test also the gradCAM approach for <code class="language-plaintext highlighter-rouge">ViT</code> models and compare the results with the previous methods quantitatively and quantitatively.</p>

<h1 id="conclusions">Conclusions</h1>

<p>In this tutorial, we have analyzed the <code class="language-plaintext highlighter-rouge">ViT</code> model and how it works. We have developed a simple <code class="language-plaintext highlighter-rouge">ViT</code> classifier for the pizza-sushi-steak dataset and trained the model. We have also analyzed two approaches for explaining the behavior of the <code class="language-plaintext highlighter-rouge">ViT</code> model. The first approach called <mark>Attention Rollout</mark> is based on the <code class="language-plaintext highlighter-rouge">Attention Maps</code> and actually a way to summarize the content of the attention maps to understand the behavior of the model. The second approach is called <mark>Gradient Attention Rollout</mark> and is based on the <code class="language-plaintext highlighter-rouge">Gradient-based</code> methods and actually a way to visualize the gradient influence over the attention maps which helps as well to understand the behavior of the model. We conclude with a simple TODO exercise that will help you understand the behavior of the <code class="language-plaintext highlighter-rouge">ViT</code> model and the interpretability methods.</p>

<h1 id="references">References</h1>
<p><a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" style="font-size: smaller" rel="external nofollow noopener" target="_blank">[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, {. Kaiser, and I. Polosukhin. Advances in Neural Information Processing Systems, page 5998‚Äì6008. (2017).</a></p>

<p><a href="https://openreview.net/pdf?id=YicbFdNTTy" style="font-size: smaller" rel="external nofollow noopener" target="_blank">[2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, N, Houlsby, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, International Conference on Learning Representations (2021).</a></p>

<p><a href="https://arxiv.org/abs/2005.00928" style="font-size: smaller" rel="external nofollow noopener" target="_blank">[3] S. Abnar, W. Zuidema, Quantifying Attention Flow in Transformers, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020.</a></p>

<p><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.pdf" style="font-size: smaller" rel="external nofollow noopener" target="_blank">[4] H. Chefer, S. Gur, L. Wolf, Transformer Interpretability Beyond Attention Visualization, CVPR, 2021.</a></p>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/"></d-bibliography>
</div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        ¬© Copyright 2024 Christos  Athanasiadis. 
      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
